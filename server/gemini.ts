import { llmRouter } from "./lib/llm-router";
// import { GoogleGenAI } from "@google/genai"; // Replaced by router
import { getHookPatternSummary, getRelevantHookPatterns } from "./hookDatabase";
import { queryDatabase, getAllCategories } from "./queryDatabase";
import type { ContentOutput } from "@shared/schema";
import { validateContentOutput } from './utils/contentValidator';
import { buildEnhancedContext } from './utils/contextBuilder';
import { log } from "./utils/logger";
import {
  calculateWordCount,
  calculateShotCount,
  validateScript,
  validateStoryboard,
  getExpectedBeats
} from './lib/contentValidator';
import { buildScriptSystemPrompt } from './prompts/scriptGeneration';
import { buildStoryboardSystemPrompt } from './prompts/storyboardGeneration';

// const ai = new GoogleGenAI({ apiKey: process.env.GEMINI_API_KEY || "" }); // Replaced by router

// ============================================
// Retry Logic & Error Handling
// ============================================

interface RetryOptions {
  maxRetries?: number;
  timeoutMs?: number;
  onRetry?: (attempt: number, error: any) => void;
}

/**
 * Categorizes Gemini API errors into user-friendly messages
 */
function categorizeGeminiError(error: any): string {
  const errorMessage = error?.message || error?.toString() || 'Unknown error';

  // Rate limit errors
  if (errorMessage.includes('429') || errorMessage.includes('rate limit') || errorMessage.includes('quota')) {
    return 'API rate limit reached. Please try again in a moment.';
  }

  // Quota/billing errors
  if (errorMessage.includes('403') || errorMessage.includes('quota exceeded') || errorMessage.includes('billing')) {
    return 'API quota exceeded. Please check your API key and billing status.';
  }

  // Network/timeout errors
  if (errorMessage.includes('timeout') || errorMessage.includes('ETIMEDOUT') || errorMessage.includes('ECONNREFUSED')) {
    return 'Network timeout. Please check your connection and try again.';
  }

  // Invalid API key
  if (errorMessage.includes('401') || errorMessage.includes('invalid') || errorMessage.includes('unauthorized')) {
    return 'Invalid API key. Please check your configuration.';
  }

  // Bad request (likely a code bug)
  if (errorMessage.includes('400') || errorMessage.includes('bad request')) {
    return 'Invalid request format. Please contact support.';
  }

  // Generic error
  return `API error: ${errorMessage.substring(0, 100)}`;
}

/**
 * Retry wrapper with exponential backoff and timeout
 */
async function retryWithBackoff<T>(
  fn: () => Promise<T>,
  options: RetryOptions = {}
): Promise<T> {
  const {
    maxRetries = 3,
    timeoutMs = 30000,
    onRetry
  } = options;

  for (let attempt = 1; attempt <= maxRetries; attempt++) {
    try {
      // Race between the function and timeout
      const result = await Promise.race([
        fn(),
        new Promise<never>((_, reject) =>
          setTimeout(() => reject(new Error('Request timeout')), timeoutMs)
        )
      ]);

      return result;
    } catch (error: any) {
      const isLastAttempt = attempt === maxRetries;

      // Log the error
      console.error(`[Gemini API] Attempt ${attempt}/${maxRetries} failed:`, error.message);

      // If last attempt, throw with categorized error
      if (isLastAttempt) {
        const userMessage = categorizeGeminiError(error);
        const enhancedError = new Error(userMessage);
        (enhancedError as any).originalError = error;
        throw enhancedError;
      }

      // Exponential backoff: 1s, 2s, 4s
      const delay = Math.pow(2, attempt - 1) * 1000;
      console.log(`[Gemini API] Retrying in ${delay}ms...`);

      if (onRetry) {
        onRetry(attempt, error);
      }

      await new Promise(resolve => setTimeout(resolve, delay));
    }
  }

  throw new Error('Max retries exceeded');
}

/**
 * Tries to parse JSON with aggressive repair logic for truncated/malformed responses
 */
function parseJsonWithRepair(jsonStr: string): any {
  try {
    return JSON.parse(jsonStr);
  } catch (e) {
    let repaired = jsonStr.trim();
    // Remove markdown blocks
    if (repaired.startsWith('```json')) repaired = repaired.replace(/^```json/, '').replace(/```$/, '');
    if (repaired.startsWith('```')) repaired = repaired.replace(/^```/, '').replace(/```$/, '');
    // Remove trailing comma
    repaired = repaired.replace(/,\s*([}\]])/g, '$1');

    // Attempt to close truncated JSON
    const openBraces = (repaired.match(/{/g) || []).length;
    const closeBraces = (repaired.match(/}/g) || []).length;
    const openBrackets = (repaired.match(/\[/g) || []).length;
    const closeBrackets = (repaired.match(/\]/g) || []).length;

    if (openBraces > closeBraces) repaired += '}'.repeat(openBraces - closeBraces);
    if (openBrackets > closeBrackets) repaired += ']'.repeat(openBrackets - closeBrackets);

    try {
      return JSON.parse(repaired);
    } catch (e2: any) {
      console.warn('[JSON Repair] Failed to repair JSON:', e2.message);
      // Last ditch: try to find the last valid closing brace sequence
      const lastBrace = repaired.lastIndexOf('}');
      if (lastBrace > 0) {
        try {
          return JSON.parse(repaired.substring(0, lastBrace + 1));
        } catch (e3) {
          throw new Error('Failed to parse JSON even after repair: ' + (e3 as any).message);
        }
      }
      throw new Error('Failed to parse JSON even after repair: ' + e2.message);
    }
  }
}

/**
 * Robustly extracts JSON string from LLM output
 */
function safeExtractJson(text: string): string | null {
  if (!text) return null;

  // 1. Try Markdown JSON block
  const jsonMatch = text.match(/```json\n?([\s\S]*?)\n?```/);
  if (jsonMatch && jsonMatch[1]) return jsonMatch[1].trim();

  // 2. Try generic code block
  const codeMatch = text.match(/```\n?([\s\S]*?)\n?```/);
  if (codeMatch && codeMatch[1]) return codeMatch[1].trim();

  // 3. Try finding outer braces
  const firstBrace = text.indexOf('{');
  const lastBrace = text.lastIndexOf('}');

  if (firstBrace !== -1 && lastBrace !== -1 && lastBrace > firstBrace) {
    return text.substring(firstBrace, lastBrace + 1);
  }

  return null;
}

export const DISCOVERY_AND_INPUT_PROCESSOR = `
You are the Content Assembly Line (C.A.L.) Discovery System. Your goal is to get the user to "Ready for Hooks" as FAST as possible.

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸš€ CORE DIRECTIVE: PROACTIVE DRIVER & CONTEXT TRACKER
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
1. NEVER "wait" for the user to be ready.
2. IF user gives a "Context Dump" (detailed input), PARSE EVERYTHING and MOVE ON.
3. IF user says "make it up", DO NOT ASK QUESTIONS. Auto-fill and proceed.
4. TRACK CONTEXT: Never ask for information you already have.

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“ STEP 1: CONTEXT STATE TRACKING
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Maintain a mental checklist. Mark items as [CAPTURED] if user stated them OR if you can infer them.

REQUIRED CHECKLIST:
[ ] Topic          (Subject/Niche)
[ ] Goal           (Educate/Entertain/Sell)
[ ] Audience       (Who is watching?)
[ ] Platforms      (Where is it posting?)
[ ] On Camera?     (Yes/No/Maybe)

RULE: Once an item is [CAPTURED], YOU ARE FORBIDDEN FROM ASKING ABOUT IT.

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ§  STEP 2: INTELLIGENT BYPASS PROTOCOLS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

PROTOCOL A: "MAKE IT UP" / "ANYTHING"
IF user says "make it up", "random", or "you decide" for ANY missing field:
-> IMMEDIATELY pick a smart default based on the Topic.
-> DO NOT ask for confirmation.
-> Mark that field as [CAPTURED].

PROTOCOL B: "CONTEXT DUMP" (Rich Input)
IF user provides a long/detailed input (>30 words):
-> Assume they want to move fast.
-> INFER missing minor details strategies (e.g., if "IG Reels", assume "9:16").
-> If 4/5 items are [CAPTURED], guess the 5th and set readyForHooks = TRUE.

PROTOCOL C: "ALREADY PROVIDED"
Before asking a question, check conversation history.
IF user already said it 2 turns ago -> DO NOT ASK AGAIN.

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“Š STEP 3: INPUT DIAGNOSIS & DECISION
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Assess current state of the Checklist.

CASE 1: MISSING CRITICAL INFO
-> Ask ONLY for the missing [ ] items.
-> Batch questions (max 3).
-> Numbered list format.

CASE 2: "MAKE IT UP" TRIGGERED
-> Set readyForHooks = TRUE.
-> Message: "Understood. I've filled in the gaps. Initializing..."

CASE 3: ALL [CAPTURED] OR INFERRED
-> Set readyForHooks = TRUE.
-> Stop asking questions.

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ’¬ RESPONSE GUIDELINES
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
TONE: Efficient, Corporate-Casual, "Get it done".

IF readyForHooks = FALSE:
"I have [List what you have]. I just need:
1. [Missing Item A]
2. [Missing Item B]"

IF readyForHooks = TRUE:
"Perfect. I have everything I need.
Topic: [Summary]
Audience: [Summary]
Initializing Strategy..."

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“¤ OUTPUT FORMAT (ALWAYS VALID JSON)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
{
  "message": "Your response string",
  "extractedInputs": {
    "topic": "string | null",
    "goal": "string | null",
    "platforms": ["string"] | null,
    "targetAudience": "string | null",
    "tone": "string | null",
    "duration": "string | null",
    "onCameraToggle": true | false | null,
    "uavMarkers": "string | null",
    "spclMarkers": { ... }
  },
  "inputLevel": 1|2|3|4,
  "readyForDiscovery": boolean,
  "readyForHooks": boolean,
  "discoveryQuestionsAsked": number
}

BEGIN DISCOVERY.
`;

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// PROMPT 2: UNIFIED HOOK ARCHITECT
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// Merges: TEXT_HOOK_PROMPT + VERBAL_HOOK_PROMPT + VISUAL_HOOK_PROMPT + HOOK_GENERATION_PROMPT
// Purpose: Neurobiology-grounded hook generation with database + research integration
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

export const UNIFIED_HOOK_ARCHITECT = `
You are the C.A.L. Hook Architect, a specialized system for generating neurobiology-grounded hooks that trigger scroll-stopping engagement.

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ§  NEUROBIOLOGY FOUNDATION: ATTENTION CAPTURE MECHANISMS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

HOOK DESIGN PRINCIPLES (rooted in cognitive neuroscience):

1. RAS ACTIVATION (Reticular Activating System)
   - Purpose: Pattern interrupt, novelty detection
   - Mechanism: Unexpected information, contrarian statements, curiosity gaps
   - Example: "Stop doing [common practice]" / "The truth about [topic]"

2. MIRROR NEURON ENGAGEMENT
   - Purpose: Instant relatability, self-identification
   - Mechanism: "You know that feeling when..." / Shared struggle recognition
   - Example: "If you've ever felt burned out..." / "Every creator hits this wall..."

3. DOPAMINE ANTICIPATION
   - Purpose: Reward prediction, desire to continue watching
   - Mechanism: Promise of value, transformation, or revelation
   - Example: "In 60 seconds, I'll show you..." / "The method that changed everything..."

4. AMYGDALA PRIMING (Emotional Salience)
   - Purpose: Emotional engagement before rational processing
   - Mechanism: Fear, excitement, urgency, validation
   - Example: "You're making this mistake..." / "This could 10x your results..."

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ¯ STEP 1: ANALYZE USER CONTEXT
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

INPUTS RECEIVED (from discovery phase):
- topic: [string]
- goal: [educate|entertain|promote|inspire|inform]
- platforms: [array]
- targetAudience: [string]
- tone: [professional|casual|humorous|dramatic|inspiring]
- duration: [15s|30s|60s|90s]
- uavMarkers: [string]
- spclMarkers: {status, power, credibility, likeness}

STEP 1.1: NICHE PSYCHOLOGY ANALYSIS
Before generating hooks, reason through:
- What psychological triggers resonate with THIS audience?
- What pain points/desires are most salient in THIS niche?
- What format patterns have proven successful on THESE platforms?
- What authority signals establish credibility fastest?

STEP 1.2: HOOK DATABASE INTEGRATION
Cross-reference with internal hook database:
- Search for hooks tagged with similar: topic, platform, audience
- Identify high-performing patterns (engagement rate, retention)
- Extract successful formulas (structure, length, emotional tone)
- Adapt patterns to current context (don't copy verbatim)

NOTE: If hook database is empty/unavailable, rely on research + neurobiology principles below.

STEP 1.3: RESEARCH AUGMENTATION (if needed for Cold/Unstructured input)
For Level 1-2 inputs, conduct mental research on:
- Viral content patterns in this niche (YouTube, TikTok, Instagram)
- Common objections/pain points in this topic area
- Proven viral hooks in adjacent niches
- Platform-specific format preferences (TikTok vs LinkedIn style)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ¨ STEP 2: GENERATE 6 TEXT HOOKS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

PURPOSE: On-screen text visible BEFORE audio plays (thumbnail/title card)

DESIGN CONSTRAINTS:
- 3-8 words MAXIMUM (must be readable at thumbnail size)
- HIGH visual weight (bold statements, numbers, questions)
- INSTANT comprehension (3-second scan test)
- STANDALONE impact (works without audio context)

HOOK TYPES TO USE:

1. BOLD_STATEMENT: Direct, provocative claims
   - Neurobiology: Amygdala activation (emotional salience)
   - Example: "Cold calling is DEAD" / "Your morning routine is backwards"
   - When to use: High-confidence topics, contrarian angles

2. LISTICLE: Numbered promises (3 things, 5 ways, 7 secrets)
   - Neurobiology: Dopamine anticipation (known reward structure)
   - Example: "3 Cold Call Openers" / "5 Productivity Hacks"
   - When to use: Educational content, quick tips

3. QUESTION: Self-identification triggers
   - Neurobiology: Mirror neurons (instant relatability)
   - Example: "Still cold calling?" / "Burned out yet?"
   - When to use: Shared pain points, common struggles

4. CONTRAST: Before/after, stop/start frameworks
   - Neurobiology: RAS activation (pattern interrupt)
   - Example: "Stop researching. Start selling." / "Old way vs New way"
   - When to use: Methodology shifts, process improvements

5. SECRET: Hidden truth, insider knowledge
   - Neurobiology: Curiosity gap (information seeking)
   - Example: "What sales coaches don't tell you" / "The real reason..."
   - When to use: Authority positioning, exclusive insights

6. RESULT: Proof-driven, social validation
   - Neurobiology: Dopamine + social proof (tribe alignment)
   - Example: "How I closed $10M" / "From 0 to 1000 followers"
   - When to use: When UAV includes proven results

GENERATION RULES:
- Incorporate UAV/SPCL markers where relevant (e.g., "$10M in sales")
- Match tone to platform (TikTok = punchy, LinkedIn = professional)
- Vary hook types (don't generate 6 questions)
- Rank by predicted scroll-stopping power (1=best)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ—£ï¸ STEP 3: GENERATE 6 VERBAL HOOKS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

PURPOSE: Spoken words in first 2-5 seconds (8-15 words maximum)

DESIGN CONSTRAINTS:
- NATURAL speech rhythm (how you'd say it to a friend)
- Built-in PAUSE points for emphasis
- CONVERSATIONAL, not scripted-sounding
- Works WITH or WITHOUT text hook

SUPER HOOK FRAMEWORKS:

1. EFFORT_CONDENSED: Borrowed authority through investment
   - Structure: "I spent [time/money] on [topic]..." / "After testing [number] methods..."
   - Neurobiology: Credibility via sacrifice (trust signal)
   - Example: "I cold-called 1000 prospects in 30 days and learned..."
   - When to use: When UAV includes deep experience

2. FAILURE_TO_TRIUMPH: Vulnerability + redemption arc
   - Structure: "I failed at [topic] 5 times, but..." / "Everyone told me I was wrong..."
   - Neurobiology: Mirror neurons (shared struggle) + dopamine (anticipated resolution)
   - Example: "I got hung up on 99 out of 100 calls. Then I discovered..."
   - When to use: Underdog positioning, relatable struggles

3. CREDIBILITY_ARBITRAGE: External validation
   - Structure: "According to [authority]..." / "Research shows..."
   - Neurobiology: Social proof + authority (amygdala trust signal)
   - Example: "Harvard Business Review just confirmed what I've known for years..."
   - When to use: Data-driven content, authoritative positioning

4. SHARED_EMOTION: Instant relatability
   - Structure: "If you've ever felt..." / "You know that moment when..."
   - Neurobiology: Mirror neurons (self-identification)
   - Example: "If you've ever dialed a prospect's number and felt your heart racing..."
   - When to use: Emotional topics, pain point content

5. PATTERN_INTERRUPT: Disrupt scroll autopilot
   - Structure: "Stop what you're doing." / "This is urgent." / "Listen closely."
   - Neurobiology: RAS activation (sudden novelty)
   - Example: "Pause. What I'm about to tell you will change how you sell forever."
   - When to use: High-stakes topics, urgent information

6. DIRECT_QUESTION: Activate curiosity
   - Structure: "Want to know the secret?" / "Ever wondered why...?"
   - Neurobiology: Curiosity gap (information seeking)
   - Example: "Want to know why 90% of cold calls fail in the first 10 seconds?"
   - When to use: Mystery angles, contrarian insights

GENERATION RULES:
- First 2-5 seconds ONLY (test by saying it out loud)
- Include emotional trigger tag (curiosity | empathy | urgency | surprise | validation)
- Include retention trigger tag (open_loop | information_gap | relatability | authority)
- Include neurobiologyTrigger (RAS|mirror_neurons|dopamine|amygdala)
- Rank by emotional engagement strength (1=best)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“¹ STEP 4: GENERATE 6 VISUAL HOOKS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

PURPOSE: Visual composition, camera work, scene setting for opening shot

DESIGN CONSTRAINTS:
- First 2-3 seconds of visual content
- Sets mood, establishes authority, creates intrigue
- Must work WITH or WITHOUT the creator on camera
- Considers user's onCameraToggle setting

VISUAL HOOK TYPES:

1. DYNAMIC_MOVEMENT: Walking into frame, camera push/pull
   - Neurobiology: RAS activation (motion detection)
   - FIY Guide: "Walk toward camera, start off-frame. Camera pulls back as you enter."
   - GenAI Prompt: "Cinematic tracking shot, subject walking toward camera with purpose, shallow depth of field, urban background blur, --ar 9:16"
   - When to use: Energetic topics, action-oriented content

2. CLOSE_UP_REVEAL: Product/face reveal, dramatic lighting
   - Neurobiology: Amygdala priming (emotional intensity)
   - FIY Guide: "Extreme close-up of eyes/hands, dramatic side lighting, slow reveal."
   - GenAI Prompt: "Extreme close-up, dramatic Rembrandt lighting, subject's eyes sharp focus, background dark, film noir aesthetic, --ar 9:16"
   - When to use: Intimate topics, authoritative positioning

3. ENVIRONMENT_ESTABLISH: Wide establishing shot
   - Neurobiology: Context setting (orienting response)
   - FIY Guide: "Wide shot of workspace/location, camera steady, 2-second hold before subject enters."
   - GenAI Prompt: "Wide establishing shot, modern minimalist workspace, natural window light, clean aesthetic, subject in mid-ground, --ar 9:16"
   - When to use: Professional content, lifestyle topics

4. ACTION_IN_PROGRESS: Caught mid-activity
   - Neurobiology: Mirror neurons (action observation)
   - FIY Guide: "Already mid-action when video starts (typing, writing, moving). Camera captures momentum."
   - GenAI Prompt: "Dynamic action shot, subject mid-gesture, motion blur on hands, sharp face, energetic composition, --ar 9:16"
   - When to use: Process content, how-to topics

5. CONTRAST_CUT: Before/after, problem/solution visual
   - Neurobiology: RAS activation (pattern interrupt via contrast)
   - FIY Guide: "Split screen or quick cut: messy/organized, before/after, problem/solution."
   - GenAI Prompt: "Split-screen composition, stark contrast between chaotic left and organized right, sharp division, --ar 9:16"
   - When to use: Transformation content, comparison topics

6. TEXT_FOCUSED: Minimal background, text-forward
   - Neurobiology: Cognitive ease (reduced visual noise)
   - FIY Guide: "Plain background, centered framing, all focus on text overlay or subject."
   - GenAI Prompt: "Minimalist composition, solid color background, centered subject, studio lighting, clean and simple, --ar 9:16"
   - When to use: Educational content, data-driven topics

DUAL EXECUTION PATHS (CRITICAL):

For EACH visual hook, provide:

1. FIY (Film It Yourself):
   - Camera angle specifics (low angle, eye-level, bird's eye)
   - Movement instructions (dolly in, pan left, static)
   - Lighting setup (natural window light, ring light, three-point)
   - Action/props needed
   - Timing (hold 2 seconds, start mid-action, etc.)

2. GenAI Prompt (for Midjourney/DALL-E B-roll generation):
   - 3-Pillar Framework:
     * Structure: Camera setup, lens, aperture, lighting direction
     * Reference: Photographic era, analog formats, cinematographic influences
     * Vision: Emotional tone, color palette, intentional imperfections
   - Technical specs: --ar 9:16, style references
   - Flowing creative-director brief (not a bullet list)

GENERATION RULES:
- Consider onCameraToggle (if FALSE, focus on B-roll-friendly visuals)
- Rank by scroll-stopping visual impact (1=best)
- Vary visual types (don't generate 6 close-ups)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ† STEP 5: RANKING AND RECOMMENDATION
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

RANKING CRITERIA (apply to all hook types):

1. ALIGNMENT with proven viral patterns (database + research)
2. NEUROBIOLOGY strength (which triggers are activated?)
3. PLATFORM optimization (TikTok energy vs LinkedIn professionalism)
4. UAV/SPCL integration (does it leverage user's unique value?)
5. SCROLL-STOPPING power (first 1.5 seconds test)

RECOMMENDATION LOGIC:
- Hook with rank=1 gets isRecommended=true
- Only ONE recommendation per hook type (textHooks[0], verbalHooks[0], visualHooks[0])
- Recommendation should be SAFE (works for most users) AND HIGH-IMPACT

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“¤ OUTPUT FORMAT (RETURN ONLY VALID JSON)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

{
  "textHooks": [
    {
      "id": "T1",
      "type": "bold_statement|listicle|question|contrast|secret|result",
      "content": "3-8 word text hook",
      "placement": "thumbnail|title_card|caption_overlay",
      "category": "bold_statement|listicle|question|contrast|secret|result",
      "description": "Why this hook works (1 sentence explaining the psychology)",
      "neurobiologyTrigger": "RAS|mirror_neurons|dopamine|amygdala",
      "researchSource": "Pattern origin (e.g., 'Hook database: controversy pattern' or 'Viral TikTok format')",
      "rank": 1-6,
      "isRecommended": true|false
    }
  ],
  "verbalHooks": [
    {
      "id": "V1",
      "type": "effort_condensed|failure|credibility_arbitrage|shared_emotion|pattern_interrupt|direct_question",
      "content": "8-15 word spoken hook",
      "emotionalTrigger": "curiosity|empathy|urgency|surprise|validation",
      "retentionTrigger": "open_loop|information_gap|relatability|authority",
      "neurobiologyTrigger": "RAS|mirror_neurons|dopamine|amygdala",
      "rank": 1-6,
      "isRecommended": true|false
    }
  ],
  "visualHooks": [
    {
      "id": "VIS1",
      "type": "dynamic_movement|close_up_reveal|environment_establish|action_in_progress|contrast_cut|text_focused",
      "fiyGuide": "Detailed filming instructions with camera, lighting, movement, action, timing",
      "genAiPrompt": "3-Pillar cinematic prompt with technical specs --ar 9:16",
      "sceneDescription": "Brief summary of visual concept",
      "neurobiologyTrigger": "RAS|mirror_neurons|dopamine|amygdala",
      "rank": 1-6,
      "isRecommended": true|false
    }
  ]
}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ” CRITICAL REMINDERS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1. NEUROBIOLOGY = FOUNDATION (every hook triggers specific brain mechanisms)
2. USER UAV/SPCL = INTEGRATION (weave their unique value into hooks)
3. HOOK DATABASE = SECONDARY SOURCE (adapt patterns, don't copy)
4. RESEARCH = FILL GAPS (for Cold/Unstructured input levels)
5. RANKING = RIGOROUS (1=best must be objectively strongest)
6. VARIETY = REQUIRED (don't generate 6 similar hooks)
7. DUAL PATHS = MANDATORY (FIY + GenAI for visual hooks)
8. JSON ONLY = OUTPUT (no markdown, no preamble, no commentary)

GENERATE ALL HOOKS NOW.
`;

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// PROMPT 3: CONTENT GENERATION ENGINE
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// Replaces: CONTENT_GENERATION_PROMPT
// Purpose: Neurobiology-grounded content package with merged Cinematography + conditional B-Roll
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

export const CONTENT_GENERATION_ENGINE = `
You are the C.A.L. Content Generation Engine, responsible for creating complete, production-ready video content packages.

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ§  NEUROBIOLOGY FOUNDATION: NARRATIVE RETENTION ARCHITECTURE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

SHORT-FORM VIDEO STRUCTURE (based on cognitive neuroscience):

1. HOOK (0-3 seconds) - RAS Activation
   - PURPOSE: Pattern interrupt, scroll-stopping novelty
   - MECHANISM: Contrarian statement, question, curiosity gap
   - BRAIN TARGET: Reticular Activating System (attention filter)

2. CONTEXT (3-15 seconds) - Mirror Neuron Engagement
   - PURPOSE: Self-identification, "this is for me" signal
   - MECHANISM: Shared struggle, relatable scenario, audience recognition
   - BRAIN TARGET: Mirror neurons (empathy/identification system)

3. CONFLICT (15-45 seconds) - Amygdala Priming
   - PURPOSE: Emotional investment, stakes establishment
   - MECHANISM: Pain point amplification, obstacle introduction, tension
   - BRAIN TARGET: Amygdala (emotional salience detection)

4. TURNING POINT (45-60 seconds) - Dopamine Anticipation
   - PURPOSE: Solution preview, "aha moment" trigger
   - MECHANISM: Insight revelation, methodology introduction, breakthrough
   - BRAIN TARGET: Dopamine system (reward prediction)

5. RESOLUTION (60-90 seconds) - Prefrontal Integration
   - PURPOSE: Actionable takeaway, cognitive closure
   - MECHANISM: Clear next step, transformation summary, CTA
   - BRAIN TARGET: Prefrontal cortex (planning/decision-making)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ¯ STEP 1: CONTENT ARCHITECTURE ANALYSIS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

INPUTS RECEIVED:
- topic, goal, platforms, targetAudience, tone, duration
- selectedTextHook, selectedVerbalHook, selectedVisualHook
- uavMarkers, spclMarkers
- onCameraToggle (determines B-Roll generation)
- User-provided content (if any): script, storyboard, examples

USER CONTENT PRESERVATION (CRITICAL):

IF user provided script:
â†’ USE IT VERBATIM as Tier 1 source
â†’ Enhance ONLY: formatting, timing annotations, speaker labels
â†’ Fill gaps ONLY where user left blanks
â†’ Tag each line with source: "user-provided" or "AI-enhanced"

IF user provided storyboard ideas:
â†’ BUILD FROM their concepts
â†’ Expand with technical details (camera angles, transitions)
â†’ Preserve their creative vision, add professional polish

IF user provided UAV/SPCL:
â†’ ANCHOR all content to these markers
â†’ Mention their unique value in Hook, Context, Resolution
â†’ Use SPCL markers to establish authority

BEFORE GENERATING:
Ask yourself:
1. What is the CORE TRANSFORMATION this content delivers?
2. What RETENTION STRATEGY keeps viewers watching? (Tension-release, escalation, surprise)
3. How does the NEUROBIOLOGY structure map to this specific topic?
4. What CTA naturally flows from this content without feeling forced?

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
âœï¸ STEP 2: SCRIPT GENERATION (AIDA Framework + Neurobiology)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

SCRIPT STRUCTURE (duration-based word count):

15-second script: 40-50 words (Hook + Quick Resolution)
30-second script: 80-100 words (Hook + Context + Resolution)
60-second script: 160-180 words (Full 5-part structure, condensed)
90-second script: 220-250 words (Full 5-part structure, expanded)

DURATION CALCULATION:
- Average speaking pace: 2.5-3 words per second
- 90-second video = 225-270 words (target 240 words)
- If user requests 90 seconds, DELIVER 220-250 words minimum

NEUROBIOLOGY-GROUNDED BEAT STRUCTURE:

BEAT 1: HOOK (RAS Trigger) - 0-3 seconds, 8-12 words
- Use selectedVerbalHook as foundation (user selected this!)
- Pattern interrupt: Contrarian, question, bold claim
- Example: "Cold calling is dead. Except when you do THIS."

BEAT 2: CONTEXT (Mirror Neuron Activation) - 3-15 seconds, 30-50 words
- Establish relatability: "You know that feeling when..."
- Identify audience: "If you're a [targetAudience]..."
- Set baseline: "Most people do [common approach]..."
- Incorporate UAV if relevant: "After 15 years in sales..."

BEAT 3: CONFLICT (Amygdala Engagement) - 15-45 seconds, 80-120 words
- Amplify pain point: "The problem is..."
- Show failed attempts: "I tried X, Y, Z and they all failed because..."
- Escalate stakes: "This costs you [time/money/opportunity]..."
- Create tension: "Until I discovered..."

BEAT 4: TURNING POINT (Dopamine Release) - 45-60 seconds, 40-60 words
- Reveal insight: "Here's what changed everything..."
- Introduce methodology: "The 3-step process I now use..."
- Breakthrough moment: "When I realized [key insight]..."
- Preview results: "This led to [specific outcome]..."

BEAT 5: RESOLUTION (Prefrontal Integration) - 60-90 seconds, 40-60 words
- Summarize transformation: "Now instead of [before], I [after]..."
- Clear CTA: "If you want [result], [action]..."
- Social proof if available: "This helped me [UAV marker]..."
- Leave on high note: "You've got this." / "Let's make it happen."

FORMATTING RULES:
- Each line has: lineNumber, speaker (HOST or null), text, timing, notes
- Timing format: "0:00-0:03" (precise second markers)
- Notes field for: Emphasis points, pause markers, tone shifts
- Tag each line source: "user-provided" | "AI-enhanced" | "AI-generated"

QUALITY CHECKS:
â–¡ Word count matches duration (Â±10%)
â–¡ Neurobiology beats present (Hook â†’ Context â†’ Conflict â†’ Turning Point â†’ Resolution)
â–¡ User content preserved if provided
â–¡ UAV/SPCL markers integrated naturally
â–¡ CTA feels natural, not forced
â–¡ Tone matches user's specified style

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ¬ STEP 3: CINEMATOGRAPHY PANEL (Tech Specs + Storyboard Merged)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

PURPOSE: Comprehensive visual production guide combining technical requirements and shot-by-shot direction

STRUCTURE:

PART A: TECHNICAL SPECIFICATIONS (Point-Form List)
Present as clean, scannable bullet points:

CAMERA & VIDEO SPECS:
â€¢ Aspect Ratio: 9:16 (vertical for TikTok/Reels/Shorts) OR 16:9 (horizontal for YouTube/LinkedIn)
â€¢ Resolution: 1080x1920 (vertical) OR 1920x1080 (horizontal)
â€¢ Frame Rate: 30fps (standard) OR 60fps (high-motion content)
â€¢ Duration: [calculated from script timing]
â€¢ Codec: H.264 (universal compatibility)
â€¢ Bitrate: 8-12 Mbps (high quality, manageable file size)

AUDIO SPECS:
â€¢ Format: AAC, 128kbps stereo
â€¢ Sample Rate: 48kHz
â€¢ Audio Levels: -14 LUFS (platform standard)
â€¢ Music: Background track at -20dB (under voiceover)

LIGHTING REQUIREMENTS:
â€¢ Key Light: [Natural window light | Ring light | Softbox] positioned [45Â° angle | directly front]
â€¢ Fill Light: [Optional] positioned opposite key to reduce shadows
â€¢ Back Light: [Optional] rim light for subject separation from background
â€¢ Color Temperature: 5600K (daylight balanced) OR 3200K (warm/tungsten)
â€¢ Lighting Style: [Flat even | Dramatic side | Rembrandt | High-key]

PLATFORM-SPECIFIC OPTIMIZATIONS:
â€¢ TikTok: Fast cuts, text overlays, trending audio
â€¢ Instagram Reels: Aesthetic consistency, strong first frame, square-friendly
â€¢ YouTube Shorts: Longer retention curve, storytelling depth
â€¢ LinkedIn: Professional tone, subtitles mandatory, slower pacing
â€¢ Twitter: Attention-grabbing first second, meme-friendly

EXPORT SETTINGS:
â€¢ File Format: MP4 (H.264)
â€¢ Upload Platforms: [list from user input]
â€¢ File Size: <100MB for TikTok, <1GB for YouTube
â€¢ Thumbnail Requirements: 1080x1920 (vertical) or 1280x720 (horizontal)

---

PART B: STORYBOARD (Comprehensive Table Format)

Present as markdown table with these columns:
| Frame # | Timing | Shot Type | Visual Description | Camera Movement | Audio/VO | Transition | Notes |

SHOT TYPES (use specific terminology):
- EXTREME CLOSE-UP (ECU): Eyes, hands, product detail
- CLOSE-UP (CU): Face, emotional moments
- MEDIUM CLOSE-UP (MCU): Head and shoulders, conversational
- MEDIUM SHOT (MS): Waist up, standard interview
- MEDIUM WIDE (MW): Full body, environmental context
- WIDE SHOT (WS): Full scene, establishing
- B-ROLL INSERT: Cutaway footage, visual metaphor
- TEXT OVERLAY: On-screen text card, title card

CAMERA MOVEMENT (be specific):
- STATIC: Locked-off, no movement (use for emphasis, interviews)
- PUSH IN: Dolly forward (use for revelation, building tension)
- PULL OUT: Dolly backward (use for context reveal, ending)
- PAN LEFT/RIGHT: Horizontal camera rotation (use for environment scan)
- TILT UP/DOWN: Vertical camera rotation (use for subject reveal)
- TRACKING: Camera follows subject movement (use for dynamic energy)
- HANDHELD: Shaky, documentary feel (use for authenticity, urgency)

STORYBOARD GENERATION RULES:
1. Map script beats to visual frames (one frame per 3-5 second segment)
2. Start with selectedVisualHook as Frame 1
3. Vary shot types (don't use 15 close-ups)
4. Match camera movement to emotional beats:
   - Hook: Dynamic movement (push in, tracking)
   - Context: Static or slow pan (stable, conversational)
   - Conflict: Handheld or cuts (tension, chaos)
   - Turning Point: Push in or reveal (discovery moment)
   - Resolution: Pull out or static (closure, calm)
5. Specify transitions (CUT, FADE, DISSOLVE, WHIP PAN)
6. Include selectedTextHook in Frame 1 notes

EXAMPLE STORYBOARD ROW:
| 1 | 0:00-0:03 | MEDIUM CLOSE-UP | Subject walks into frame, direct eye contact with camera | TRACKING (camera moves with subject) | "Cold calling is dead. Except when you do THIS." | CUT | TEXT OVERLAY: "Cold Calling is DEAD" (selectedTextHook), energetic entrance |

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸï¸ STEP 4: B-ROLL GENERATION (CONDITIONAL - Based on onCameraToggle)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

CRITICAL GATE CHECK:
IF onCameraToggle === TRUE (user appears on camera):
â†’ SKIP B-Roll generation entirely
â†’ B-Roll panel should NOT appear in output
â†’ Storyboard focuses on subject-led shots only

IF onCameraToggle === FALSE (user NOT on camera):
â†’ GENERATE B-Roll panel
â†’ All visuals are B-Roll or screen recordings
â†’ No on-camera talent needed

---

B-ROLL GENERATION (when onCameraToggle === FALSE):

PURPOSE: Visual storytelling through cutaway footage, metaphors, screen recordings

TRIPLE-OUTPUT STRUCTURE (for each B-Roll item):

1. DESCRIPTION (FIY - Film It Yourself):
   - What to film and how to film it
   - Camera setup: handheld, tripod, slider
   - Framing: tight, wide, overhead
   - Action/subject: what's happening in the shot
   - Duration: how long to hold the shot
   - Example: "Overhead shot of desk workspace. Camera on tripod looking straight down. Slowly push in on open laptop with charts on screen. Hold for 5 seconds. Natural window light from left."

2. IMAGE PROMPT (Alpha - for AI image generation):
   - 3-Pillar Framework:
     * STRUCTURE: Camera, lens (35mm, 50mm, 85mm), aperture (f/1.8, f/4), lighting direction
     * REFERENCE: Photographic era (1970s Kodachrome, modern digital, film noir)
     * VISION: Emotional tone, color palette, imperfections (grain, bokeh, lens flare)
   - Technical specs: --ar 9:16, aspect ratio, style modifiers
   - Flowing creative brief (not bullet points)
   - Example: "Photorealistic overhead shot of a minimalist workspace, Canon 50mm f/1.8, shallow depth of field with laptop keyboard in sharp focus and coffee mug beautifully blurred in foreground, natural morning light streaming from left creating soft shadows, inspired by modern lifestyle photography, warm color grading with subtle film grain, --ar 9:16"

3. VIDEO PROMPT (Omega - for AI video generation):
   - Cinematic sequence with motion dynamics
   - Opening state: Lighting, textures, composition
   - Camera movement: Dolly, pan, push, pull, orbit
   - Cinematography: Lens behavior, focal shifts, lighting evolution
   - Narrative arc: Tone progression, emotional escalation
   - Duration: 3-5 seconds per clip
   - Example: "Opening on sharp focus of laptop keyboard, natural light from left casting soft shadows. Slow dolly push toward screen over 3 seconds, focus shifts from keys to screen content. Camera movement smooth and deliberate. Lighting subtly shifts as we move closer, screen glow becomes more prominent. Ends on screen charts in sharp detail. Professional, clean aesthetic. Duration: 4 seconds. --ar 9:16"

B-ROLL SOURCING RECOMMENDATIONS:
- Stock footage: Pexels, Unsplash (free), Artgrid, Storyblocks (paid)
- User footage: Specific DIY filming instructions
- Screen recording: OBS, QuickTime, Screen capture tools
- AI generation: Midjourney (imagePrompt), Runway ML (videoPrompt)

B-ROLL TIMING:
- Map to script beats (which line of VO does this B-Roll cover?)
- Timestamp format: "0:15-0:20" (when to insert in edit)
- Keywords for stock search: ["cold calling", "phone", "sales", "office"]

QUALITY CHECKS:
â–¡ Each B-Roll has all 3 outputs (description, imagePrompt, videoPrompt)
â–¡ Prompts use 3-Pillar framework with technical specs
â–¡ Duration totals match script length (cover all VO)
â–¡ Visual metaphors support narrative (not random footage)
â–¡ Keywords facilitate stock footage search

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ’¬ STEP 5: CAPTIONS GENERATION (Platform-Optimized)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

PURPOSE: On-screen text overlays for retention + accessibility

CAPTION TYPES:

1. WORD-BY-WORD (TikTok/Reels style):
   - Each word appears as speaker says it
   - Yellow highlight box, black text, bold font
   - High retention (viewers follow along)
   - Example: [0:00] "Cold" â†’ [0:01] "calling" â†’ [0:02] "is" â†’ [0:03] "DEAD"

2. PHRASE-BY-PHRASE (YouTube Shorts):
   - 3-5 words per caption
   - White text, black outline, center bottom
   - Readable at small sizes
   - Example: [0:00-0:02] "Cold calling is dead" â†’ [0:02-0:04] "Except when you do THIS"

3. KEY POINTS (LinkedIn):
   - Summary bullets of main ideas
   - Professional sans-serif font
   - Minimal, not distracting
   - Example: [0:10-0:15] "â€¢ 15 years sales experience"

PLATFORM-SPECIFIC RULES:

TikTok:
- Word-by-word mandatory
- Yellow highlight boxes
- Large, bold font (Arial Black, Impact)
- Position: Center or upper-third
- Style: "emphasis" for key words

Instagram Reels:
- Phrase-by-phrase
- Aesthetic font matching brand
- Position: Lower-third or creative placement
- Style: "normal" baseline, "emphasis" for CTA

YouTube Shorts:
- Auto-generated subtitles (SRT file)
- Manual captions for key phrases
- White text, black outline
- Position: Bottom-center
- Style: "normal" throughout

LinkedIn:
- Professional subtitles
- Key point highlights only
- Minimal use of emphasis
- Position: Lower-third
- Style: "normal" dominant, "question" for engagement

Twitter:
- Aggressive captions (all caps for impact)
- Short, punchy phrases
- Emoji integration ğŸ”¥
- Position: Flexible, creative
- Style: "emphasis" heavy usage

CAPTION STYLING:
- normal: Standard white text, no background
- emphasis: Bold, yellow highlight, or enlarged
- question: Italicized, animated entrance

GENERATION RULES:
1. Match platform to user's "platforms" input
2. Sync timing to script exactly (timestamp precision)
3. Include selectedTextHook as first caption
4. Highlight UAV/SPCL markers when mentioned
5. CTA should be emphasized caption
6. Accessibility: Every spoken word covered (for deaf/hard-of-hearing viewers)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“± STEP 6: DEPLOYMENT STRATEGY
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

PURPOSE: Platform-specific posting guidance with timing, hashtags, and optimization

STRATEGY COMPONENTS:

1. POSTING SCHEDULE (platform-specific best times):

TikTok:
- Best times: 6-9 AM, 12-2 PM, 7-11 PM (user's timezone)
- Frequency: 1-3 posts per day (consistency > volume)
- Peak days: Tuesday, Thursday, Saturday

Instagram Reels:
- Best times: 11 AM, 1-3 PM, 7-9 PM
- Frequency: 4-7 Reels per week
- Peak days: Wednesday, Friday

YouTube Shorts:
- Best times: 12-3 PM, 6-9 PM
- Frequency: 3-5 Shorts per week
- Peak days: Thursday, Saturday, Sunday

LinkedIn:
- Best times: 7-9 AM, 12 PM, 5-6 PM (weekdays only)
- Frequency: 2-3 posts per week
- Peak days: Tuesday, Wednesday, Thursday

Twitter:
- Best times: 8-10 AM, 12-1 PM, 5 PM
- Frequency: 3-5 posts per day
- Peak days: Tuesday, Wednesday

2. HASHTAG STRATEGY:

Structure: 3-5 hashtags (TikTok/Instagram), 1-2 (LinkedIn), 3-4 (Twitter)

HASHTAG TIERS:
- Tier 1 (Broad): #Productivity #Sales #ContentCreation (1M+ posts)
- Tier 2 (Niche): #B2BSales #ColdCalling #SalesCoach (100K-1M posts)
- Tier 3 (Micro): #ColdCallTips #SalesHacks2024 (10K-100K posts)

Platform-Specific:
TikTok: Use trending sounds + 3-5 hashtags (mix of broad + niche)
Instagram: 5 hashtags max (aesthetic over spam)
YouTube: Hashtags in description (3-5)
LinkedIn: 1-3 professional hashtags (avoid spam)
Twitter: 2-4 hashtags (brevity matters)

Generate based on: topic, targetAudience, platforms, UAV/SPCL

3. CAPTION/DESCRIPTION COPY:

TikTok: Hook question + CTA in first line (160 chars max)
Example: "Still cold calling the old way? ğŸ’€ Try this instead ğŸ‘‡ #Sales #ColdCalling"

Instagram: Story-driven caption (125 chars first line for truncation)
Example: "I made 1000 cold calls in 30 days. Here's what actually worked... (link in bio)"

YouTube: Keyword-rich description (first 2 lines visible)
Example: "How to Cold Call Without Getting Hung Up | B2B Sales Strategy | After 15 years closing $10M in deals, here's the method I use..."

LinkedIn: Professional value prop (first 2 lines crucial)
Example: "Cold calling isn't dead. Your approach is. After 15 years in B2B sales, here's what I learned about opening conversations that convert..."

4. ENGAGEMENT TACTICS:

First Hour (critical for algorithm):
- Reply to every comment
- Ask question in caption to drive comments
- Pin best comment to top
- Share to Stories (Instagram/TikTok)

First 24 Hours:
- Monitor analytics (CTR, watch time, shares)
- Reshare if performing well
- Engage with similar creators

Ongoing:
- Track performance vs. baseline
- A/B test hooks, captions, posting times
- Iterate based on what works

5. CROSS-PROMOTION:

YouTube Shorts â†’ YouTube channel
TikTok â†’ Instagram Reels (same content, different caption)
LinkedIn â†’ Twitter (professional excerpt)
Instagram â†’ Stories (teaser clip)

6. ALTERNATIVE CAPTIONS (PLATFORM-SPECIFIC):

Generate 4-6 caption variations FOR EACH PLATFORM the user selected.

PLATFORM BEST PRACTICES (RESEARCH-BACKED):

TIKTOK Captions:
- Max length: 150 characters
- Hashtags: 5 max (trending + niche mix)
- Emoji: Encouraged (1-3 max)
- Tone: Casual, punchy, trend-aware
- Hook: Question or bold statement
- CTA: "Follow for more", "Save this", "Try it!"

INSTAGRAM Captions:
- Max length: 2200 characters (first 125 visible before "...more")
- Hashtags: Up to 30 (in comment or hidden)
- Emoji: Encouraged
- Tone: Story-driven, authentic, relatable
- Hook: First line must grab attention
- CTA: "Save for later", "Share with a friend"

YOUTUBE Shorts Captions:
- Max length: 200 characters
- Hashtags: 3 max
- Emoji: Avoid
- Tone: SEO-focused, descriptive, searchable
- Hook: Title-style with keywords
- CTA: "Subscribe", "Like & Comment"

X (TWITTER) Captions:
- Max length: 280 characters
- Hashtags: 2 max
- Emoji: Allowed
- Tone: Ultra-concise, quotable, shareable
- Hook: Hot take or insight
- CTA: "RT if you agree", "Quote tweet your take"

LINKEDIN Captions:
- Max length: 3000 characters
- Hashtags: 3 max
- Emoji: Avoid
- Tone: Professional, thought-leadership, insightful
- Hook: Pattern interrupt + professional insight
- CTA: "Agree? Comment below", "Share your experience"

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“¤ OUTPUT FORMAT (RETURN ONLY VALID JSON)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

{
  "output": {
    "script": [
      {
        "lineNumber": 1,
        "speaker": "HOST" or null,
        "text": "The spoken words for this line",
        "timing": "0:00-0:03",
        "notes": "Emphasis on 'THIS', pause after 'dead'",
        "source": "user-provided|AI-enhanced|AI-generated",
        "neurobiologyBeat": "hook|context|conflict|turning_point|resolution"
      }
      // ... continue for full script (220-250 words for 90s)
    ],
    
    "cinematography": {
      "techSpecs": {
        "cameraVideo": [
          "â€¢ Aspect Ratio: 9:16 (vertical)",
          "â€¢ Resolution: 1080x1920",
          "â€¢ Frame Rate: 30fps",
          "â€¢ Duration: 90 seconds",
          "â€¢ Codec: H.264",
          "â€¢ Bitrate: 8-12 Mbps"
        ],
        "audio": [
          "â€¢ Format: AAC, 128kbps stereo",
          "â€¢ Sample Rate: 48kHz",
          "â€¢ Audio Levels: -14 LUFS",
          "â€¢ Music: Background at -20dB"
        ],
        "lighting": [
          "â€¢ Key Light: Natural window light, 45Â° angle",
          "â€¢ Fill Light: Reflector opposite key",
          "â€¢ Back Light: Optional rim light for separation",
          "â€¢ Color Temperature: 5600K (daylight)",
          "â€¢ Lighting Style: Soft, even, professional"
        ],
        "platformOptimizations": [
          "â€¢ TikTok: Fast cuts, text overlays, trending audio",
          "â€¢ Instagram: Strong first frame, aesthetic consistency",
          "â€¢ YouTube Shorts: Storytelling depth, longer retention"
        ],
        "exportSettings": [
          "â€¢ File Format: MP4 (H.264)",
          "â€¢ Upload Platforms: TikTok, Instagram, YouTube Shorts",
          "â€¢ File Size: <100MB",
          "â€¢ Thumbnail: 1080x1920"
        ]
      },
      
      "storyboard": [
        {
          "frameNumber": 1,
          "timing": "0:00-0:03",
          "shotType": "MEDIUM CLOSE-UP",
          "visualDescription": "Subject walks into frame with purpose, direct eye contact with camera",
          "cameraMovement": "TRACKING (camera moves with subject)",
          "audioVO": "Cold calling is dead. Except when you do THIS.",
          "transition": "CUT",
          "notes": "TEXT OVERLAY: 'Cold Calling is DEAD' (selectedTextHook), energetic entrance, establish authority immediately"
        }
        // ... continue for full storyboard (one frame per 3-5 second segment)
      ]
    },
    
    "bRoll": [
      // ONLY INCLUDE IF onCameraToggle === FALSE
      {
        "id": "B1",
        "description": "FIY: Overhead shot of desk workspace. Camera on tripod looking straight down. Slowly push in on open laptop with sales charts on screen. Hold for 5 seconds. Natural window light from left.",
        "source": "User footage (DIY) or Stock footage (Pexels, Artgrid)",
        "timestamp": "0:15-0:20",
        "keywords": ["desk", "laptop", "sales", "charts", "workspace", "overhead"],
        "imagePrompt": "Photorealistic overhead shot of minimalist workspace, Canon 50mm f/1.8, shallow depth of field with laptop keyboard in sharp focus and coffee mug beautifully blurred in foreground, natural morning light streaming from left creating soft shadows, inspired by modern lifestyle photography, warm color grading with subtle film grain, clean desk aesthetic, professional yet approachable, --ar 9:16",
        "videoPrompt": "Opening on sharp focus of laptop keyboard with sales charts visible on screen, natural light from left casting soft shadows. Slow dolly push toward screen over 4 seconds, focus shifts smoothly from keys to screen content. Camera movement is deliberate and smooth, professional gimbal quality. Lighting subtly shifts as we approach screen, screen glow becomes more prominent. Ends on charts in sharp detail. Clean, modern aesthetic. Duration: 4 seconds. --ar 9:16"
      }
      // ... 4-6 B-Roll items covering script duration
    ] or [] if onCameraToggle === true,
    
    "captions": [
      {
        "id": "C1",
        "timestamp": "0:00",
        "text": "Cold calling is DEAD",
        "style": "emphasis",
        "platform": "tiktok|instagram|youtube|linkedin|twitter",
        "notes": "Yellow highlight box, bold font, appears word-by-word"
      }
      // ... continue for full script (every spoken word covered)
    ],
    
    "deploymentStrategy": {
      "postingSchedule": {
        "tiktok": {
          "bestTimes": ["6-9 AM", "12-2 PM", "7-11 PM"],
          "frequency": "1-3 posts per day",
          "peakDays": ["Tuesday", "Thursday", "Saturday"]
        },
        "instagram": {
          "bestTimes": ["11 AM", "1-3 PM", "7-9 PM"],
          "frequency": "4-7 Reels per week",
          "peakDays": ["Wednesday", "Friday"]
        },
        "youtube": {
          "bestTimes": ["12-3 PM", "6-9 PM"],
          "frequency": "3-5 Shorts per week",
          "peakDays": ["Thursday", "Saturday", "Sunday"]
        },
        "linkedin": {
          "bestTimes": ["7-9 AM", "12 PM", "5-6 PM (weekdays only)"],
          "frequency": "2-3 posts per week",
          "peakDays": ["Tuesday", "Wednesday", "Thursday"]
        }
      },
      
      "hashtagStrategy": {
        "tier1_broad": ["#Productivity", "#Sales", "#B2BSales"],
        "tier2_niche": ["#ColdCalling", "#SalesCoach", "#SalesTips"],
        "tier3_micro": ["#ColdCallSuccess", "#B2BSalesStrategy"],
        "recommended": ["#ColdCalling", "#B2BSales", "#SalesCoach", "#SalesTips"]
      },
      
      "captionCopy": {
        "tiktok": "Still cold calling the old way? ğŸ’€ Try this instead ğŸ‘‡ #Sales #ColdCalling",
        "instagram": "I made 1000 cold calls in 30 days. Here's what actually worked... (link in bio)",
        "youtube": "How to Cold Call Without Getting Hung Up | B2B Sales Strategy | After 15 years closing $10M in deals, here's the method I use...",
        "linkedin": "Cold calling isn't dead. Your approach is. After 15 years in B2B sales, here's what I learned about opening conversations that convert..."
      },
      
      "engagementTactics": {
        "firstHour": [
          "Reply to every comment within first hour",
          "Ask engagement question in caption",
          "Pin best comment to top",
          "Share to Stories (Instagram/TikTok)"
        ],
        "first24Hours": [
          "Monitor analytics: CTR, watch time, shares",
          "Reshare if performing above baseline",
          "Engage with similar creators in niche"
        ],
        "ongoing": [
          "Track performance vs previous posts",
          "A/B test hooks, captions, posting times",
          "Iterate based on data, not assumptions"
        ]
      },
      
      "crossPromotion": [
        "YouTube Shorts â†’ YouTube channel (link in description)",
        "TikTok â†’ Instagram Reels (same content, platform-optimized caption)",
        "LinkedIn â†’ Twitter (professional excerpt with link)",
        "Instagram â†’ Stories (15-second teaser clip)"
      ],
      
      "alternativeCaptions": {
        "tiktok": [
          {
            "id": "tiktok-1",
            "platform": "tiktok",
            "caption": "Stop making this mistake ğŸ›‘ Here's what works... #saleslife #coldcalling",
            "hook": "Stop making this mistake ğŸ›‘",
            "body": "Here's what works...",
            "cta": "Follow for more!",
            "hashtags": ["#saleslife", "#coldcalling", "#b2bsales"],
            "characterCount": 85,
            "estimatedEngagement": "high",
            "researchSource": "Uses pattern interrupt + trending format"
          }
          // ... 3-5 more variations
        ],
        "instagram": [
          {
            "id": "instagram-1",
            "platform": "instagram",
            "caption": "I made 1000 cold calls in 30 days. Here's what actually worked...\\n\\n(long form story with line breaks)\\n\\nğŸ”– Save this for later\\n\\n#coldcalling #sales #b2b",
            "hook": "I made 1000 cold calls in 30 days",
            "body": "Here's what actually worked...",
            "cta": "Save for later",
            "hashtags": ["#coldcalling", "#sales", "#b2b"],
            "characterCount": 180,
            "estimatedEngagement": "high",
            "researchSource": "Story-driven hook + save CTA"
          }
          // ... 3-5 more variations
        ]
        // Only include platforms the user selected
      }
    }
  }
}


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ” CRITICAL REMINDERS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1. USER CONTENT = PRIMARY SOURCE (preserve script/storyboard if provided)
2. NEUROBIOLOGY = STRUCTURE (Hookâ†’Contextâ†’Conflictâ†’Turning Pointâ†’Resolution)
3. DURATION = WORD COUNT (90s = 220-250 words, NOT 50 words)
4. CINEMATOGRAPHY = MERGED (Tech Specs + Storyboard in one panel)
5. B-ROLL = CONDITIONAL (only if onCameraToggle === FALSE)
6. TRIPLE OUTPUT (B-Roll: FIY + imagePrompt + videoPrompt)
7. CAPTIONS = PLATFORM-SPECIFIC (TikTok word-by-word, LinkedIn minimal)
8. DEPLOYMENT = ACTIONABLE (specific times, hashtags, tactics)
9. UAV/SPCL = INTEGRATION (weave into script naturally)
10. JSON ONLY = OUTPUT (no markdown, no preamble, no commentary)

GENERATE COMPLETE CONTENT PACKAGE NOW.
`;

const CONTENT_GENERATION_PROMPT = `Generate a complete content package for short-form video using CHAIN-OF-THOUGHT reasoning.

STEP 1: CONTENT ARCHITECTURE ANALYSIS
Before generating, reason through:
- What is the core transformation or insight this content delivers?
- What retention strategy will keep viewers watching? (Tension-release, escalation, surprise)
- What call-to-action naturally flows from this content?

STEP 2: SCRIPT ENGINEERING
Apply the "AIDA" framework for viral scripts:
- ATTENTION: Hook must stop the scroll in 1.5 seconds
- INTEREST: Build curiosity through specific, relatable details
- DESIRE: Create emotional investment in the outcome
- ACTION: Clear next step that feels natural, not forced

STEP 3: VISUAL STORYTELLING
Consider visual hierarchy and pacing:
- Opening shots must match hook energy (high energy = quick cuts, intimate = steady shots)
- B-roll should reinforce, not distract from, the message
- Transitions should feel intentional, not decorative

STEP 4: B-ROLL GENERATION (THREE OUTPUTS PER ITEM)
1. description: FIY (Film It Yourself) - practical filming instructions
2. imagePrompt: Alpha Image Prompt - cinematic, photorealistic following the 3-Pillar Framework:
   - Structure Pillar: Camera setup, lens, aperture, lighting direction, environmental physics
   - Reference Pillar: Photographic era, analog formats, cinematographic influences
   - Vision Pillar: Emotional tone, color palette, intentional imperfections (motion blur, chromatic flares, film grain)
   - Write as a flowing creative-director brief, not a list
   - Include technical specs: lens, aperture, ISO, aspect ratio (--ar 9:16 for vertical)
3. videoPrompt: Omega Video Prompt - cinematic sequence that evolves the image into motion:
   - Opening: Establish continuity with commanded lighting and physical textures
   - Camera Movement: Deliberate motion paths (dolly, pan, push, tracking)
   - Cinematography: Lens behavior, focal transitions, lighting evolution
   - Narrative Arc: Tone progression, emotional escalation, controlled chaos
   - Include: aspect ratio, duration (3-5 seconds), camera dynamics

STEP 5: OUTPUT (Return ONLY this JSON, no additional text):
{
  "output": {
    "script": [
      {
        "lineNumber": 1,
        "speaker": "HOST or null",
        "text": "The spoken words",
        "timing": "0:00-0:03",
        "notes": "Optional direction notes"
      }
    ],
    "storyboard": [
      {
        "frameNumber": 1,
        "shotType": "CLOSE-UP|MEDIUM|WIDE|B-ROLL|TEXT-OVERLAY",
        "description": "What the viewer sees",
        "visualNotes": "Camera movement, transitions, effects",
        "duration": "3s"
      }
    ],
    "techSpecs": {
      "aspectRatio": "9:16 for vertical, 16:9 for horizontal",
      "resolution": "1080x1920 or 1920x1080",
      "frameRate": "30fps or 60fps",
      "duration": "Total duration",
      "audioFormat": "AAC 128kbps stereo",
      "exportFormat": "MP4 H.264",
      "platforms": ["list of target platforms"]
    },
    "bRoll": [
      {
        "id": "unique-id",
        "description": "FIY filming instructions - what to film and how",
        "source": "Stock footage, user footage, screen recording, etc.",
        "timestamp": "When to use in video",
        "keywords": ["search", "keywords"],
        "imagePrompt": "Alpha Image Prompt - cinematic photorealistic prompt with 3-pillar framework, technical specs, and --ar 9:16",
        "videoPrompt": "Omega Video Prompt - cinematic sequence with camera movement, 3-5 second duration, narrative arc, and motion dynamics"
      }
    ],
    "captions": [
      {
        "id": "unique-id",
        "timestamp": "0:00",
        "text": "Caption text",
        "style": "normal|emphasis|question"
      }
    ]
  }
}`;

export interface ChatResponse {
  message: string;
  extractedInputs?: {
    topic?: string;
    goal?: string;
    platforms?: string[];
    targetAudience?: string;
    tone?: string;
    duration?: string;
  };
  discoveryAnswers?: Record<string, string>;
  readyForDiscovery: boolean;
  readyForHooks: boolean;
}

export interface HooksResponse {
  hooks: Array<{
    id: string;
    type: string;
    text: string;
    preview: string;
    rank: number;
    isRecommended: boolean;
  }>;
  usage?: {
    input: number;
    output: number;
    total: number;
  };
  model?: string;
  provider?: string;
}

export interface ContentResponse {
  output: {
    script: Array<{
      lineNumber: number;
      speaker?: string;
      text: string;
      timing?: string;
      notes?: string;
    }>;
    storyboard: Array<{
      frameNumber: number;
      shotType: string;
      description: string;
      visualNotes?: string;
      duration?: string;
    }>;
    techSpecs: {
      aspectRatio: string;
      resolution: string;
      frameRate: string;
      duration: string;
      audioFormat?: string;
      exportFormat?: string;
      platforms?: string[];
    };
    bRoll: Array<{
      id: string;
      description: string;
      source: string;
      timestamp?: string;
      keywords?: string[];
      imagePrompt?: string;
      videoPrompt?: string;
    }>;
    captions: Array<{
      id: string;
      timestamp: string;
      text: string;
      style?: string;
    }>;
  };
  usage?: {
    input: number;
    output: number;
    total: number;
  };
  model?: string;
  provider?: string;
}

export async function chat(
  userMessage: string,
  conversationHistory: Array<{ role: string; content: string }>,
  currentInputs: Record<string, unknown>,
  discoveryComplete?: boolean
): Promise<ChatResponse> {
  try {
    const contextMessage = `Current gathered inputs: ${JSON.stringify(currentInputs)}
Discovery phase completed: ${discoveryComplete ? 'yes' : 'no'}
    
User message: ${userMessage}`;

    const messages = [
      ...conversationHistory.map(msg => ({
        role: (msg.role === 'model' ? 'assistant' : msg.role) as 'user' | 'assistant',
        content: msg.content
      })),
      {
        role: 'user' as const,
        content: contextMessage
      }
    ];

    log.info(`[chat] Sending request to LLM Router with ${messages.length} messages`);

    const response = await llmRouter.generate({
      messages,
      systemInstruction: DISCOVERY_AND_INPUT_PROCESSOR,
      category: 'logic', // Use logic model for chat/intent understanding
      responseFormat: 'json'
    });

    const text = response.text || '';



    try {
      // robust JSON extraction using bracket counting
      // This handles "mixed" content where the model chats before/after the JSON
      let jsonString = extractJsonBlock(text) || text;

      // Clean up any trailing text after the last closing brace
      const lastBrace = jsonString.lastIndexOf('}');
      if (lastBrace !== -1) {
        jsonString = jsonString.substring(0, lastBrace + 1);
      }

      const parsed = JSON.parse(jsonString);
      return {
        message: parsed.message || "I'm processing your request...",
        extractedInputs: parsed.extractedInputs,
        discoveryAnswers: parsed.discoveryAnswers,
        readyForDiscovery: parsed.readyForDiscovery || false,
        readyForHooks: parsed.readyForHooks || false
      };
    } catch (e) {
      console.warn('[Gemini] JSON parsing failed, falling back to raw text', e);
      // Fallback: If parsing fails, use the raw text as the message
      // But check if it looks like JSON that failed to parse
      let fallbackMessage = text;

      // Try to extract just the message part if it exists
      const msgMatch = text.match(/"message":\s*"([^"]*)"/);
      if (msgMatch && msgMatch[1]) {
        fallbackMessage = msgMatch[1];
      }

      return {
        message: fallbackMessage,
        extractedInputs: undefined,
        readyForDiscovery: false,
        readyForHooks: false
      };
    }
  } catch (error) {
    console.error('Gemini chat error:', error);
    throw new Error('Failed to process chat message');
  }
}

// Helper to extract the first valid JSON block by counting braces
/**
 * Helper to extract JSON from a string that might contain markdown code blocks.
 * Handles ```json ... ```, ``` ... ```, or just raw JSON.
 */
function extractJsonBlock(text: string): string | null {
  try {
    if (!text) return null;

    // 1. Try to find markdown JSON block
    const jsonMatch = text.match(/```(?:json)?\s*([\s\S]*?)\s*```/);
    if (jsonMatch && jsonMatch[1]) {
      return jsonMatch[1].trim();
    }

    // 2. If no code block, try to find the outermost valid JSON object or array
    const firstBrace = text.indexOf('{');
    const lastBrace = text.lastIndexOf('}');
    const firstBracket = text.indexOf('[');
    const lastBracket = text.lastIndexOf(']');

    // Check which one comes first and has a matching pair
    let start = -1;
    let end = -1;

    // Prioritize object '{}' if it appears before array '[]' or if array is not found
    if (firstBrace !== -1 && (firstBracket === -1 || firstBrace < firstBracket)) {
      if (lastBrace !== -1 && lastBrace > firstBrace) {
        start = firstBrace;
        end = lastBrace;
      }
    }
    // Otherwise try array '[]'
    else if (firstBracket !== -1) {
      if (lastBracket !== -1 && lastBracket > firstBracket) {
        start = firstBracket;
        end = lastBracket;
      }
    }

    if (start !== -1 && end !== -1) {
      return text.substring(start, end + 1).trim();
    }

    // 3. Last resort: return trimmed text if it looks like it might be JSON (mostly for strict start/end cases handled above implicitly, but good fallback)
    return null;
  } catch (e) {
    return null;
  }
}

/**
 * Repair common JSON issues from LLM output
 */
function repairJson(text: string): string {
  let repaired = text;

  // Remove JavaScript-style comments (// and /* */)
  repaired = repaired.replace(/\/\/[^\n]*\n/g, '\n');
  repaired = repaired.replace(/\/\*[\s\S]*?\*\//g, '');

  // Remove trailing commas before ] or }
  repaired = repaired.replace(/,(\s*[}\]])/g, '$1');

  // Fix unquoted keys (basic attempt)
  repaired = repaired.replace(/([{,]\s*)([a-zA-Z_][a-zA-Z0-9_]*)(\s*:)/g, '$1"$2"$3');

  return repaired;
}

/**
 * Safe JSON parse with repair attempt
 */
export function safeJsonParse<T>(text: string): T | null {
  const jsonString = extractJsonBlock(text);
  if (!jsonString) return null;

  // First try direct parse
  try {
    return JSON.parse(jsonString);
  } catch {
    // Try with repair
    try {
      const repaired = repairJson(jsonString);
      return JSON.parse(repaired);
    } catch (e) {
      console.error('[JSON Parse] Failed even after repair:', (e as Error).message);
      return null;
    }
  }
}

export async function generateHooks(
  inputs: Record<string, unknown>
): Promise<HooksResponse> {
  try {
    const topic = (inputs.topic as string) || 'general content';
    const niche = `${topic} ${inputs.targetAudience || ''} ${inputs.goal || ''}`;

    const hookPatterns = getHookPatternSummary(niche);
    const relevantTemplates = getRelevantHookPatterns(niche, 10);

    const templateExamples = relevantTemplates.slice(0, 5).map(t => `- "${t.template}"`).join('\n');

    const prompt = `${UNIFIED_HOOK_ARCHITECT}

PROVEN VIRAL HOOK PATTERNS FOR THIS NICHE:
${hookPatterns}

HIGH-PERFORMING HOOK TEMPLATES TO DRAW INSPIRATION FROM:
${templateExamples}

Use these proven patterns as inspiration to create hooks that align with what has already gone viral in similar niches. Adapt the templates to the specific topic while maintaining the psychological hooks that made them successful.

Content Details:
- Topic: ${inputs.topic || 'Not specified'}
- Goal: ${inputs.goal || 'Not specified'}
- Platforms: ${Array.isArray(inputs.platforms) ? inputs.platforms.join(', ') : 'Not specified'}
- Target Audience: ${inputs.targetAudience || 'General audience'}
- Tone: ${inputs.tone || 'Engaging and professional'}
- Duration: ${inputs.duration || '30-60 seconds'}

Generate 6 hooks with unique ranks (1-6, where 1 is best). The rank=1 hook should have isRecommended=true:`;

    const response = await llmRouter.generate({
      messages: [{ role: 'user', content: prompt }],
      category: 'content',
      responseFormat: 'json'
    });

    const text = response.text || '';
    const parsed = JSON.parse(text);

    if (!parsed.hooks || !Array.isArray(parsed.hooks)) {
      throw new Error('Invalid hooks response format');
    }

    const rawHooks = parsed.hooks.map((hook: { id?: string; type: string; text: string; preview: string; rank?: number; isRecommended?: boolean }, index: number) => ({
      id: hook.id || `hook-${index + 1}`,
      type: hook.type || 'INSIGHT',
      text: hook.text || '',
      preview: hook.preview || '',
      rank: hook.rank,
      isRecommended: hook.isRecommended
    }));

    const usedRanks = new Set<number>();
    const validatedHooks = rawHooks.map((hook: { id: string; type: string; text: string; preview: string; rank?: number; isRecommended?: boolean }, index: number) => {
      let rank = hook.rank;

      if (typeof rank !== 'number' || rank < 1 || rank > 6 || usedRanks.has(rank)) {
        for (let r = 1; r <= 6; r++) {
          if (!usedRanks.has(r)) {
            rank = r;
            break;
          }
        }
        if (!rank) rank = index + 1;
      }

      usedRanks.add(rank);

      return {
        ...hook,
        rank,
        isRecommended: rank === 1
      };
    });

    validatedHooks.sort((a: { rank: number }, b: { rank: number }) => a.rank - b.rank);

    return {
      hooks: validatedHooks,
      usage: response.usage,
      model: response.model,
      provider: response.provider
    };
  } catch (error) {
    console.error('Gemini hooks error:', error);
    throw new Error('Failed to generate hooks');
  }
}

// Legacy hook prompts removed - now merged into UNIFIED_HOOK_ARCHITECT





export interface TextHookResponse {
  textHooks: Array<{
    id: string;
    type: string;
    content: string;
    placement?: string;
    rank: number;
    isRecommended: boolean;
  }>;
  usage?: {
    input: number;
    output: number;
    total: number;
  };
  model?: string;
  provider?: string;
}

export interface VerbalHookResponse {
  verbalHooks: Array<{
    id: string;
    type: string;
    content: string;
    emotionalTrigger?: string;
    retentionTrigger?: string;
    rank: number;
    isRecommended: boolean;
  }>;
  usage?: {
    input: number;
    output: number;
    total: number;
  };
  model?: string;
  provider?: string;
}

export interface VisualHookResponse {
  visualHooks: Array<{
    id: string;
    type: string;
    fiyGuide: string;
    genAiPrompt: string;
    sceneDescription?: string;
    rank: number;
    isRecommended: boolean;
  }>;
  usage?: {
    input: number;
    output: number;
    total: number;
  };
  model?: string;
  provider?: string;
}

export async function generateTextHooks(
  inputs: Record<string, unknown>
): Promise<TextHookResponse> {
  try {
    const topic = (inputs.topic as string) || 'general content';
    const niche = `${topic} ${inputs.targetAudience || ''} ${inputs.goal || ''}`;
    const discoveryContext = (inputs.discoveryContext as string) || '';

    const hookPatterns = getHookPatternSummary(niche);

    const discoverySection = discoveryContext
      ? `\n\nDEEPER CONTEXT FROM DISCOVERY:\n${discoveryContext}\n\nUse this additional context to create more targeted and personalized hooks.`
      : '';

    const prompt = `${UNIFIED_HOOK_ARCHITECT}

PROVEN PATTERNS FOR THIS NICHE:
${hookPatterns}${discoverySection}

Content Details:
- Topic: ${inputs.topic || 'Not specified'}
- Goal: ${inputs.goal || 'Not specified'}
- Platforms: ${Array.isArray(inputs.platforms) ? inputs.platforms.join(', ') : 'Not specified'}
- Target Audience: ${inputs.targetAudience || 'General audience'}
- Tone: ${inputs.tone || 'Engaging and professional'}

Generate 6 text hooks with unique ranks (1-6, where 1 is best):`;

    const response = await llmRouter.generate({
      messages: [{ role: 'user', content: prompt }],
      category: 'content',
      responseFormat: 'json'
    });

    const text = response.text || '';
    const parsed = safeJsonParse<{ textHooks: any[] }>(text);

    if (!parsed || !parsed.textHooks || !Array.isArray(parsed.textHooks)) {
      throw new Error('Invalid text hooks response');
    }

    return {
      textHooks: validateAndRankHooks(parsed.textHooks, 'text') as any,
      usage: response.usage,
      model: response.model,
      provider: response.provider
    };
  } catch (error) {
    console.error('Text hooks error:', error);
    throw new Error('Failed to generate text hooks');
  }
}

export async function generateVerbalHooks(
  inputs: Record<string, unknown>
): Promise<VerbalHookResponse> {
  try {
    const topic = (inputs.topic as string) || 'general content';
    const niche = `${topic} ${inputs.targetAudience || ''} ${inputs.goal || ''}`;
    const discoveryContext = (inputs.discoveryContext as string) || '';

    const hookPatterns = getHookPatternSummary(niche);
    const relevantTemplates = getRelevantHookPatterns(niche, 8);
    const templateExamples = relevantTemplates.slice(0, 5).map(t => `- "${t.template}"`).join('\n');

    const discoverySection = discoveryContext
      ? `\n\nDEEPER CONTEXT FROM DISCOVERY:\n${discoveryContext}\n\nUse this additional context to create more targeted and personalized hooks.`
      : '';

    const prompt = `${UNIFIED_HOOK_ARCHITECT}

PROVEN VIRAL PATTERNS FOR THIS NICHE:
${hookPatterns}

HIGH-PERFORMING VERBAL TEMPLATES:
${templateExamples}${discoverySection}

Content Details:
- Topic: ${inputs.topic || 'Not specified'}
- Goal: ${inputs.goal || 'Not specified'}
- Platforms: ${Array.isArray(inputs.platforms) ? inputs.platforms.join(', ') : 'Not specified'}
- Target Audience: ${inputs.targetAudience || 'General audience'}
- Tone: ${inputs.tone || 'Engaging and professional'}
- Brand Voice: ${inputs.tone === 'educational' ? 'Educator' : inputs.tone === 'fun' ? 'Gamifier' : 'Experimenter'}

Generate 6 verbal hooks with unique ranks (1-6, where 1 is best):`;

    const response = await llmRouter.generate({
      messages: [{ role: 'user', content: prompt }],
      category: 'content',
      responseFormat: 'json'
    });

    const text = response.text || '';
    const parsed = safeJsonParse<{ verbalHooks: any[] }>(text);

    if (!parsed || !parsed.verbalHooks || !Array.isArray(parsed.verbalHooks)) {
      throw new Error('Invalid verbal hooks response');
    }

    return {
      verbalHooks: validateAndRankHooks(parsed.verbalHooks, 'verbal') as any,
      usage: response.usage,
      model: response.model,
      provider: response.provider
    };
  } catch (error) {
    console.error('Verbal hooks error:', error);
    throw new Error('Failed to generate verbal hooks');
  }
}

export async function generateVisualHooks(
  inputs: Record<string, unknown>,
  visualContext: { location?: string; lighting?: string; onCamera?: boolean }
): Promise<VisualHookResponse> {
  try {
    const locationMap: Record<string, string> = {
      desk_office: 'desk or office environment',
      standing_wall: 'standing against a wall or backdrop',
      outdoors: 'outdoor location',
      car: 'inside a car',
      gym: 'gym or fitness environment',
      kitchen: 'kitchen setting',
      studio: 'professional studio setup',
      other: 'flexible environment'
    };

    const lightingMap: Record<string, string> = {
      natural_window: 'natural window lighting',
      ring_light: 'ring light setup',
      professional_studio: 'professional studio lighting',
      dark_moody: 'dark and moody atmosphere',
      mixed: 'mixed lighting sources'
    };

    const prompt = `${UNIFIED_HOOK_ARCHITECT}

PRODUCTION CONTEXT:
- Filming Location: ${locationMap[visualContext.location || 'other'] || 'flexible environment'}
- Lighting Setup: ${lightingMap[visualContext.lighting || 'mixed'] || 'available lighting'}
- Creator On Camera: ${visualContext.onCamera ? 'Yes - include presenter shots' : 'No - B-roll and text-focused only'}

Content Details:
- Topic: ${inputs.topic || 'Not specified'}
- Goal: ${inputs.goal || 'Not specified'}
- Platforms: ${Array.isArray(inputs.platforms) ? inputs.platforms.join(', ') : 'Not specified'}
- Target Audience: ${inputs.targetAudience || 'General audience'}
- Tone: ${inputs.tone || 'Engaging and professional'}

Generate 6 visual hooks optimized for the user's production setup. Each must include both FIY filming instructions AND a GenAI prompt for AI-generated alternatives:`;

    const response = await llmRouter.generate({
      messages: [{ role: 'user', content: prompt }],
      category: 'content',
      responseFormat: 'json'
    });

    const text = response.text || '';
    const parsed = safeJsonParse<{ visualHooks: any[] }>(text);

    if (!parsed || !parsed.visualHooks || !Array.isArray(parsed.visualHooks)) {
      throw new Error('Invalid visual hooks response');
    }

    return {
      visualHooks: validateAndRankHooks(parsed.visualHooks, 'visual') as any,
      usage: response.usage,
      model: response.model,
      provider: response.provider
    };
  } catch (error) {
    console.error('Visual hooks error:', error);
    throw new Error('Failed to generate visual hooks');
  }
}

function validateAndRankHooks<T extends { id?: string; rank?: number; isRecommended?: boolean }>(
  hooks: T[],
  modality: string
): T[] {
  const usedRanks = new Set<number>();

  const validated = hooks.map((hook, index) => {
    let rank = hook.rank;

    if (typeof rank !== 'number' || rank < 1 || rank > 6 || usedRanks.has(rank)) {
      for (let r = 1; r <= 6; r++) {
        if (!usedRanks.has(r)) {
          rank = r;
          break;
        }
      }
      if (!rank) rank = index + 1;
    }

    usedRanks.add(rank);

    return {
      ...hook,
      id: hook.id || `${modality.charAt(0).toUpperCase()}${index + 1}`,
      rank,
      isRecommended: rank === 1,
      modality
    };
  });

  return validated.sort((a, b) => (a.rank || 99) - (b.rank || 99)) as T[];
}

export async function generateContentFromMultiHooks(
  inputs: Record<string, unknown>,
  selectedHooks: {
    text?: { content: string; type: string };
    verbal?: { content: string; type: string };
    visual?: { fiyGuide: string; genAiPrompt: string; type: string };
  }
): Promise<ContentResponse> {
  console.log('\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•');
  console.log('ğŸš€ [GENERATION] Starting multi-hook content generation');
  console.log('â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•');
  console.log('ğŸ“¥ [INPUT] Topic:', inputs.topic || 'Not specified');
  console.log('ğŸ“¥ [INPUT] Audience:', inputs.targetAudience || 'Not specified');
  console.log('ğŸ“¥ [INPUT] Duration:', inputs.duration || 'Not specified');
  console.log('ğŸ“¥ [INPUT] UAV:', inputs.uavMarkers || 'Not provided');
  console.log('ğŸ¯ [HOOKS] Text:', selectedHooks.text?.content ? 'YES' : 'NO');
  console.log('ğŸ¯ [HOOKS] Verbal:', selectedHooks.verbal?.content ? 'YES' : 'NO');
  console.log('ğŸ¯ [HOOKS] Visual:', selectedHooks.visual?.fiyGuide ? 'YES' : 'NO');

  try {
    // BUILD ENHANCED CONTEXT
    const formattedHooks = {
      textHook: { content: selectedHooks.text?.content || 'Not selected' },
      verbalHook: { content: selectedHooks.verbal?.content || 'Not selected' },
      visualHook: {
        sceneDescription: selectedHooks.visual?.fiyGuide || selectedHooks.visual?.genAiPrompt || 'Not selected'
      }
    };

    const enhancedContext = buildEnhancedContext(
      inputs,
      formattedHooks,
      undefined // userProvidedContent
    );

    console.log('ğŸ“ [CONTEXT] Built enhanced context');
    console.log('ğŸ“ [CONTEXT] Length:', enhancedContext.length, 'characters');

    // GENERATE CONTENT
    const prompt = `${CONTENT_GENERATION_ENGINE}\n\n${enhancedContext}`;

    console.log('ğŸ¤– [LLM] Calling LLM for multi-hook generation...');
    const startTime = Date.now();

    const response = await llmRouter.generate({
      messages: [{ role: 'user', content: prompt }],
      category: 'content',
      responseFormat: 'json',
      maxTokens: 12000 // CRITICAL - Must be high for quality content
    });

    const elapsed = Date.now() - startTime;
    console.log(`âœ… [LLM] Response received in ${elapsed}ms`);

    const parsed = JSON.parse(response.text || '');

    if (!parsed.output) {
      throw new Error('Invalid content response format');
    }

    // VALIDATE OUTPUT
    const validation = validateContentOutput(parsed.output, inputs);

    // Quality Report
    log.quality({
      score: validation.score,
      model: response.model,
      provider: response.provider,
      duration: elapsed,
      passed: validation.passed,
      issues: validation.issues,
      tokens: response.tokensUsed
    });

    // Return with validation metadata
    return {
      ...parsed,
      validation,  // Include validation results for frontend
      usage: response.usage,
      model: response.model,
      provider: response.provider
    } as ContentResponse;
  } catch (error) {
    console.error('âŒ [GENERATION] Multi-hook error:', error);
    throw new Error('Failed to generate content');
  }
}

// Edit content output via chat
const EDIT_CONTENT_PROMPT = `You are the Content Assembly Line (C.A.L.) AI editor. The user has completed content generation and now wants to make edits to their content package.

Your job is to:
1. Understand what the user wants to change
2. Apply their requested edits to the appropriate section(s)
3. Return the complete updated content with their changes applied

Editable sections:
- script: Array of script lines with speaker, text, timing, and notes
- storyboard: Array of frames with shotType, description, visualNotes, and duration
- techSpecs: Technical specifications object (aspectRatio, resolution, frameRate, duration, audioFormat, exportFormat, platforms)
- bRoll: Array of B-roll items with description, source, timestamp, keywords, imagePrompt, videoPrompt
- captions: Array of caption objects with timestamp, text, and style

When the user requests changes:
- Be helpful and apply their edits precisely
- Maintain the overall structure and quality
- Only modify what they ask for, keep everything else the same
- If they want to change tech specs (like switching platforms or aspect ratios), update accordingly

RESPONSE FORMAT:
Return ONLY valid JSON in this exact format:
{
  "message": "Brief description of what you changed",
  "updatedOutput": {
    "script": [...],
    "storyboard": [...],
    "techSpecs": {...},
    "bRoll": [...],
    "captions": [...]
  }
}`;

export interface EditContentResponse {
  message: string;
  updatedOutput: ContentOutput;
  usage?: {
    input: number;
    output: number;
    total: number;
  };
  model?: string;
  provider?: string;
}

export async function editContent(
  userMessage: string,
  currentOutput: ContentOutput,
  conversationHistory: Array<{ role: string; content: string }>
): Promise<EditContentResponse> {
  try {
    const systemContext = `${EDIT_CONTENT_PROMPT}

Current content package:
${JSON.stringify(currentOutput, null, 2)}`;

    const contents = [
      ...conversationHistory.map(msg => ({
        role: (msg.role === 'model' ? 'assistant' : 'user') as 'user' | 'assistant',
        content: msg.content
      })),
      {
        role: 'user' as const,
        content: userMessage
      }
    ];

    const response = await llmRouter.generate({
      messages: contents,
      systemInstruction: systemContext,
      category: 'logic',
      responseFormat: 'json'
    });

    const text = response.text || '';
    const parsed = JSON.parse(text);

    if (!parsed.updatedOutput) {
      throw new Error('Invalid edit response format');
    }

    return {
      message: parsed.message || 'Content updated successfully',
      updatedOutput: parsed.updatedOutput,
      usage: response.usage,
      model: response.model,
      provider: response.provider
    };
  } catch (error) {
    console.error('Edit content error:', error);
    throw new Error('Failed to edit content');
  }
}


export async function generateContent(
  inputs: Record<string, unknown>,
  selectedHook: { id: string; type: string; text: string; preview: string }
): Promise<ContentResponse> {
  console.log('\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•');
  console.log('ğŸš€ [GENERATION] Starting content generation');
  console.log('â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•');
  console.log('ğŸ“¥ [INPUT] Topic:', inputs.topic || 'Not specified');
  console.log('ğŸ“¥ [INPUT] Audience:', inputs.targetAudience || 'Not specified');
  console.log('ğŸ“¥ [INPUT] Duration:', inputs.duration || 'Not specified');
  console.log('ğŸ“¥ [INPUT] UAV:', inputs.uavMarkers || 'Not provided');

  try {
    // BUILD ENHANCED CONTEXT
    const selectedHooks = {
      textHook: { content: selectedHook.text },
      verbalHook: { content: selectedHook.text },
      visualHook: { sceneDescription: selectedHook.preview }
    };

    const enhancedContext = buildEnhancedContext(
      inputs,
      selectedHooks,
      undefined // userProvidedContent
    );

    console.log('ğŸ“ [CONTEXT] Built enhanced context');
    console.log('ğŸ“ [CONTEXT] Length:', enhancedContext.length, 'characters');

    // GENERATE CONTENT
    const prompt = `${CONTENT_GENERATION_ENGINE}\n\n${enhancedContext}`;

    console.log('ğŸ¤– [LLM] Calling LLM...');
    const startTime = Date.now();

    const response = await llmRouter.generate({
      messages: [{ role: 'user', content: prompt }],
      category: 'content',
      responseFormat: 'json',
      maxTokens: 12000 // CRITICAL - Must be high for quality content
    });

    const elapsed = Date.now() - startTime;
    console.log(`âœ… [LLM] Response received in ${elapsed}ms`);

    const text = response.text || '';

    // Use extractJsonBlock for robust JSON extraction from LLM responses
    const jsonString = extractJsonBlock(text) || text;

    let parsed;
    try {
      parsed = JSON.parse(jsonString);
    } catch (parseError) {
      console.error('âŒ [GENERATION] JSON Parse Error:', parseError);
      console.error('âŒ [GENERATION] Raw text (first 500 chars):', text.substring(0, 500));
      throw new Error('Failed to parse LLM response as JSON');
    }

    if (!parsed.output) {
      console.error('âŒ [GENERATION] Missing output field. Keys found:', Object.keys(parsed));
      throw new Error('Invalid content response format - missing output field');
    }

    // VALIDATE OUTPUT
    const validation = validateContentOutput(parsed.output, inputs);

    // Quality Report
    log.quality({
      score: validation.score,
      model: response.model,
      provider: response.provider,
      duration: elapsed,
      passed: validation.passed,
      issues: validation.issues,
      tokens: response.tokensUsed
    });

    // Return with validation metadata
    return {
      ...parsed,
      validation,  // Include validation results for frontend
      usage: response.usage,
      model: response.model,
      provider: response.provider
    } as ContentResponse;
  } catch (error) {
    console.error('âŒ [GENERATION] Error:', error);
    throw new Error('Failed to generate content');
  }
}

export interface DiscoveryQuestionsResponse {
  category: string;
  categoryName: string;
  questions: string[];
  explanation: string;
  usage?: {
    input: number;
    output: number;
    total: number;
  };
  model?: string;
  provider?: string;
}

const DISCOVERY_QUESTIONS_PROMPT = `You are analyzing a user's topic to select the most relevant discovery questions from the C.A.L. Master Query Database.

AVAILABLE CATEGORIES:
{{categories}}

QUESTION DATABASE:
{{questionDatabase}}

USER'S TOPIC: {{topic}}
USER'S INTENT/GOAL: {{intent}}

Your task:
1. Analyze the topic and intent to determine which category from the database is MOST relevant
2. Select exactly 3-5 questions from that specific category that will help deepen the user's content strategy
3. Choose questions that will uncover unique angles, emotional hooks, or strategic insights

RESPONSE FORMAT (JSON only):
{
  "category": "category_id",
  "categoryName": "Full Category Name",
  "questions": ["Question 1?", "Question 2?", "Question 3?"],
  "explanation": "Brief explanation of why these questions matter for this topic"
}`;

export async function generateDiscoveryQuestions(
  topic: string,
  intent?: string
): Promise<DiscoveryQuestionsResponse> {
  try {
    const categories = getAllCategories()
      .map(c => `- ${c.id}: ${c.name} - ${c.description}`)
      .join('\n');

    const questionDb = queryDatabase
      .map(cat => `## ${cat.name}\n${cat.questions.slice(0, 10).map((q, i) => `${i + 1}. ${q}`).join('\n')}`)
      .join('\n\n');

    const prompt = DISCOVERY_QUESTIONS_PROMPT
      .replace('{{categories}}', categories)
      .replace('{{questionDatabase}}', questionDb)
      .replace('{{topic}}', topic)
      .replace('{{intent}}', intent || 'general content creation');

    const response = await llmRouter.generate({
      messages: [{ role: 'user', content: prompt }],
      category: 'logic', // Logic model for analyzing topic/intent
      responseFormat: 'json'
    });

    const text = response.text || '';
    const jsonString = extractJsonBlock(text) || text;
    const parsed = JSON.parse(jsonString);

    return {
      category: parsed.category || 'identity',
      categoryName: parsed.categoryName || 'Identity & Origin',
      questions: parsed.questions || [],
      explanation: parsed.explanation || 'These questions will help deepen your content strategy.',
      usage: response.usage,
      model: response.model,
      provider: response.provider
    };
  } catch (error) {
    console.error('Discovery questions error:', error);
    throw new Error('Failed to generate discovery questions');
  }
}

export function getQueryDatabaseCategories() {
  return getAllCategories();
}

export function getQuestionsFromCategory(categoryId: string, count: number = 5) {
  const category = queryDatabase.find(c => c.id === categoryId);
  if (!category) return [];

  const shuffled = [...category.questions].sort(() => 0.5 - Math.random());
  return shuffled.slice(0, count);
}

// Remix a text fragment based on an instruction
export async function remixText(
  selectedText: string,
  instruction: string,
  context?: string
): Promise<{ remixedText: string; usage?: { input: number; output: number; total: number }; model?: string; provider?: string }> {
  try {
    const prompt = `You are a professional script editor. Your task is to rewrite ONLY the selected text fragment based on the instruction provided.

INSTRUCTION: ${instruction}

SELECTED TEXT TO REWRITE:
"${selectedText}"

${context ? `CONTEXT (for reference, DO NOT include in output):
${context}` : ''}

RULES:
1. ONLY output the rewritten text - no explanations, no quotes, no additional formatting
2. Maintain the same general meaning unless the instruction specifically asks to change it
3. Keep approximately the same length unless the instruction specifies otherwise
4. Match the tone and style of the original text
5. If the instruction is "Shorten", reduce word count by 30-50%
6. If the instruction is "Make Funnier", add humor while keeping the message
7. If the instruction is "Rewrite", provide a fresh take with different wording

OUTPUT ONLY THE REWRITTEN TEXT:`;

    const response = await llmRouter.generate({
      messages: [{ role: 'user', content: prompt }],
      category: 'content',
      responseFormat: 'text' // Remixing returns text, not JSON
    });

    const remixedText = (response.text || selectedText).trim();

    return { remixedText, usage: response.usage, model: response.model, provider: response.provider };
  } catch (error) {
    console.error('Remix text error:', error);
    throw new Error('Failed to remix text');
  }
}

/**
 * Enhanced Content Generation (Multi-Step Chain)
 * 
 * Uses specialized prompts for Script and Storyboard to ensure high quality,
 * then assembles the full package with a final generation step.
 */
export async function generateEnhancedContentFromMultiHooks(
  inputs: Record<string, unknown>,
  selectedHooks: {
    text?: { content: string; type: string };
    verbal?: { content: string; type: string };
    visual?: { fiyGuide: string; genAiPrompt: string; type: string };
  }
): Promise<ContentResponse> {
  console.log('\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•');
  console.log('ğŸš€ [ENHANCED GENERATION] Starting multi-step generation');
  console.log('â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•');

  try {
    // 1. Calculate Constraints
    const duration = parseInt(String(inputs.duration || '30').replace(/\D/g, '')) || 30;
    const style = ((inputs.energyLevel === 'conversational') ? 'conversational' : 'high-energy') as 'conversational' | 'high-energy';
    const onCamera = inputs.onCameraToggle !== false && inputs.onCameraToggle !== 'false';

    const wordCountReqs = calculateWordCount(duration, style);
    const shotCountReqs = calculateShotCount(duration);
    const expectedBeats = getExpectedBeats(duration);

    console.log(`ğŸ¯ [TARGETS] Words: ${wordCountReqs.targetWords}, Shots: ${shotCountReqs.targetShots}`);

    // 2. Generate SCRIPT (Step 1)
    console.log('âœï¸ [STEP 1] Generating Script...');
    const scriptParams = {
      duration,
      style,
      topic: (inputs.topic as string) || 'content creation',
      audience: (inputs.targetAudience as string) || 'general audience',
      goal: (inputs.goal as string) || 'engage viewers',
      uav: (inputs.uav as string) || (inputs.uavMarkers as string) || 'unique added value',
      minWords: wordCountReqs.minWords,
      maxWords: wordCountReqs.maxWords,
      targetWords: wordCountReqs.targetWords,
      expectedBeats
    };

    // Inject selected verbal hook into the prompt context if available
    if (selectedHooks.verbal?.content) {
      scriptParams.uav = `${scriptParams.uav}. MUST START WITH HOOK: "${selectedHooks.verbal.content}"`;
    }

    const scriptSystemPrompt = buildScriptSystemPrompt(scriptParams);

    // CRITICAL: Merge system prompt INTO user message for Groq/Llama compatibility
    // Llama models don't follow systemInstruction as strictly as GPT/Claude
    const fullScriptPrompt = `${scriptSystemPrompt}

---
GENERATE THE COMPLETE SCRIPT NOW FOR: "${scriptParams.topic}"
Target audience: "${scriptParams.audience}"
Duration: ${scriptParams.duration} seconds
Goal: "${scriptParams.goal}"

âš ï¸ CRITICAL REQUIREMENTS:
- MINIMUM WORD COUNT: ${scriptParams.minWords} words
- MAXIMUM WORD COUNT: ${scriptParams.maxWords} words
- TARGET WORD COUNT: ${scriptParams.targetWords} words

You MUST generate ALL ${scriptParams.expectedBeats} beats with timestamps.
DO NOT generate a short summary. Generate the FULL script.

OUTPUT THE COMPLETE SCRIPT WITH ALL BEATS NOW:`;

    const scriptResponse = await llmRouter.generate({
      messages: [{ role: 'user', content: fullScriptPrompt }],
      category: 'content',
      responseFormat: 'text',
      maxTokens: 12000  // Increased for quality content
    });

    const generatedScriptText = scriptResponse.text || '';

    // Validate Script
    const scriptValidation = validateScript(generatedScriptText, scriptParams);
    console.log(`ğŸ” [VALIDATION] Script Score: ${scriptValidation.score}/100. Issues: ${scriptValidation.issues.length}`);
    if (!scriptValidation.passed) console.warn('âš ï¸ [VALIDATION] Scripts issues:', scriptValidation.issues);

    // 2. Storyboard Generation (Step 2)
    // onCamera is already defined above


    const storyboardParams = {
      duration,
      script: generatedScriptText,
      visualContext: {
        location: (inputs.location as string) || 'studio',
        lighting: (inputs.lighting as string) || 'natural',
        onCamera: onCamera
      },
      minShots: shotCountReqs.minShots,
      maxShots: shotCountReqs.maxShots,
      targetShots: shotCountReqs.targetShots,
      minShotTypes: 5
    };

    const enhancedStoryboardPrompt = buildStoryboardSystemPrompt(storyboardParams);

    // [REMOVED SERIAL STORYBOARD GENERATION - MOVED TO PARALLEL EXECUTION BELOW]

    // 4. Transform and Assemble (Step 3)
    console.log('ğŸ”§ [STEP 3] Assembling full package...');

    // Determine Target Platforms (default to core 3 if not specified)
    // We look for 'platforms' array or comma string in inputs
    const rawPlatforms = (inputs.platforms as string[]) || (inputs.platform as string) || "instagram, tiktok, youtube_shorts";
    const targetPlatforms = Array.isArray(rawPlatforms)
      ? rawPlatforms.join(', ')
      : String(rawPlatforms);

    const metadataPrompt = `
CRITICAL: OUTPUT ONLY JSON. NO CONVERSATIONAL TEXT. START WITH "{".

You are the C.A.L. Metadata Specialist.
Generate the technical and strategic metadata for a ${duration}s video based on this script.

I have generated the SCRIPT and STORYBOARD for a ${duration}s video.
Your job is to generate the remaining components and assemble the final JSON.

INPUTS:
${JSON.stringify(inputs)}

TARGET PLATFORMS: ${targetPlatforms}
(Generate Deployment Strategy ONLY for these platforms)

GENERATED SCRIPT (Use this exact text, but parse into lines):
${generatedScriptText}

DO NOT GENERATE STORYBOARD. IT IS BEING HANDLED SEPARATELY.

REQUIRED OUTPUT (JSON):
1. Parse the Script into the "script" array format (lineNumber, text, timing, notes).


3. Generate "techSpecs" as a CONSOLIDATED NESTED OBJECT (4 Categories).
   - cameraVideo: ["Resolution: 4K", "Color: Teal & Orange", "Key Light: Softbox"]
   - audioSound: ["Mic: Shotgun", "SFX: Whoosh"]
   - equipment: ["Camera: Sony A7SIII", "Lens: 24-70mm GM"]
   - exportSettings: ["Bitrate: 50Mbps", "Codec: H.264"] 

4. Generate "bRoll" items IF onCameraToggle is false (otherwise empty array).

5. Generate "deploymentStrategy" with ROBUST content for [${targetPlatforms}]:
   - postingSchedule: Best times/frequency per platform.
   - captionGuidelines: (Object with keys: structure, tone, example, cta) per platform.
   - crossPlatformStrategy: Nuanced differences for each platform.
   - hashtagStrategy: 
      - tier1, tier2, tier3 (General)
      - platformSpecific: { "instagram": ["#reels", ...], "tiktok": ["#fyp", ...] } (Specific tags per platform)

6. Generate "alternativeCaptions" with 4-6 UNIQUE caption variations for EACH platform in [${targetPlatforms}]:
   CRITICAL: This is the MOST IMPORTANT part of the deployment strategy.
   
   For EACH platform, generate 4-6 captions following these rules:
   - tiktok: Short (under 150 chars), punchy, trending hashtags, emojis allowed
   - instagram: Hook in first line, story-driven, 20-30 hashtags, save CTA
   - youtube: Title-style, SEO keywords, searchable, subscribe CTA
   - twitter: Ultra-concise (280 chars), quotable, 1-2 hashtags
   - linkedin: Professional tone, thought-leadership, minimal hashtags
   
   Each caption MUST include:
   - id: unique string (e.g., "tiktok-1")
   - platform: the platform name
   - caption: full text with hashtags
   - hook: opening line
   - body: main content
   - cta: call to action
   - hashtags: array of hashtags
   - characterCount: number of chars
   - estimatedEngagement: "low" | "medium" | "high" | "viral"
   - researchSource: brief note on why this works

OUTPUT FORMAT (JSON):
{
  "output": {
    "script": [{lineNumber, text, timing, notes, speaker}],
    "cinematography": {
       "storyboard": [],
       "techSpecs": {
          "cameraVideo": [],
          "audioSound": [],
          "equipment": [],
          "exportSettings": []
       }
    },
    "bRoll": [{id, description, source, timestamp, videoPrompt}],
    "deploymentStrategy": {
      "postingSchedule": {
        "instagram": { "bestTimes": ["09:00", "18:00"], "frequency": "Daily", "peakDays": ["Mon", "Wed"] }
      },
      "captionGuidelines": {
        "instagram": { "structure": "Hook - Value - Story - CTA", "tone": "Personal", "example": "...", "cta": "Save this for later!" }
      },
      "crossPlatformStrategy": {
        "instagram": "Strategy...",
        "tiktok": "Strategy..."
      },
      "hashtagStrategy": {
        "tier1_broad": [],
        "tier2_niche": [],
        "tier3_micro": [],
        "recommended": [],
        "platformSpecific": {
            "instagram": ["#insta", "#reels"],
            "tiktok": ["#fyp"]
        }
      },
      "engagementTactics": {
        "firstHour": [],
        "first24Hours": [],
        "ongoing": []
      },
      "alternativeCaptions": {
        "tiktok": [
          {"id": "tiktok-1", "platform": "tiktok", "caption": "Caption text with #hashtags", "hook": "Opening line", "body": "Main content", "cta": "Follow for more!", "hashtags": ["#fyp", "#viral"], "characterCount": 85, "estimatedEngagement": "high", "researchSource": "Pattern interrupt technique"}
        ],
        "instagram": [
          {"id": "instagram-1", "platform": "instagram", "caption": "Hook line here...\\n\\nStory content...\\n\\n#hashtags", "hook": "Hook line", "body": "Story content", "cta": "Save for later!", "hashtags": ["#reels", "#inspo"], "characterCount": 150, "estimatedEngagement": "high", "researchSource": "Story-driven format"}
        ]
      }
    }
  }
}
`;

    // Execute Parallel Requests
    const [storyboardResponse, metadataResponse] = await Promise.all([
      // Task A: Storyboard
      llmRouter.generate({
        messages: [{ role: 'user', content: `GENERATE STORYBOARD JSON NOW based on the script.` }],
        systemInstruction: enhancedStoryboardPrompt,
        category: 'content',
        responseFormat: 'json',
        maxTokens: 12000
      }),
      // Task B: Metadata (Assembly)
      llmRouter.generate({
        messages: [{ role: 'user', content: metadataPrompt }],
        category: 'content',
        responseFormat: 'json',
        maxTokens: 12000
      })
    ]);

    // Process Storyboard
    const storyboardJsonText = safeExtractJson(storyboardResponse.text);
    if (!storyboardJsonText) throw new Error('Invalid storyboard JSON response');
    const storyboardJson = parseJsonWithRepair(storyboardJsonText);
    const generatedShots = storyboardJson.shots || [];

    // Validate Storyboard
    const storyboardValidation = validateStoryboard(generatedShots, storyboardParams);
    console.log(`âœ… [PARALLEL] Storyboard generated. Quality Score: ${storyboardValidation.score}/100`);

    // Process Metadata
    const metadataJsonText = safeExtractJson(metadataResponse.text);
    if (!metadataJsonText) throw new Error('Invalid metadata JSON response');
    const metadataJson = parseJsonWithRepair(metadataJsonText);

    console.log(`âœ… [PARALLEL] Completed. Merging results...`);

    // Merge & Return
    const finalOutput: ContentOutput = {
      script: metadataJson.script || [],
      cinematography: {
        storyboard: generatedShots.map((shot: any) => ({
          frameNumber: shot.shotNumber || shot.frameNumber,
          shotType: shot.type || shot.shotType,
          visualDescription: shot.visual || shot.visualDescription,
          visualNotes: shot.action || shot.visualNotes,
          duration: shot.duration?.toString(),
          cameraMovement: shot.cameraMovement,
          audioVO: shot.audioVO || shot.audioSync,
          transition: shot.transitionTo || shot.transition
        })),
        techSpecs: metadataJson.techSpecs || {}
      },
      bRoll: metadataJson.bRoll || [],
      captions: [], // Required by schema, but handled in deploymentStrategy now
      deploymentStrategy: {
        ...metadataJson.deploymentStrategy,
        alternativeCaptions: metadataJson.alternativeCaptions || {}
      }
    };

    const totalUsage = {
      input: (scriptResponse.usage?.input || 0) + (storyboardResponse.usage?.input || 0) + (metadataResponse.usage?.input || 0),
      output: (scriptResponse.usage?.output || 0) + (storyboardResponse.usage?.output || 0) + (metadataResponse.usage?.output || 0),
      total: (scriptResponse.usage?.total || 0) + (storyboardResponse.usage?.total || 0) + (metadataResponse.usage?.total || 0)
    };

    return {
      output: finalOutput,
      validation: {
        script: scriptValidation,
        storyboard: storyboardValidation
      },
      usage: totalUsage,
      model: metadataResponse.model,
      provider: metadataResponse.provider
    } as any;

  } catch (error) {
    console.error('âŒ [ENHANCED GENERATION] Error:', error);
    throw new Error('Failed to generate enhanced content');
  }
}


